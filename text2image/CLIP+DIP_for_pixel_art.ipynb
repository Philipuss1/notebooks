{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CLIP+DIP for pixel art\n",
        "Original Author: Daniel Russell ([@danielrussruss](https://twitter.com/danielrussruss))\n",
        "\n",
        "This notebook uses [OpenAI's CLIP](https://github.com/openai/CLIP) model to optimize the weights of the [Deep Image Prior](https://github.com/DmitryUlyanov/deep-image-prior) skip network to output an image that matches a text prompt. \n",
        "\n",
        "This is a somewhat minimal implementation intended for research and artistic exploration. Do not use this project or a derivative for commercial work.\n",
        "\n",
        "This notebook would not be possible without the foundational work established by [Ryan Murdock](https://twitter.com/advadnoun) and [Katherine Crowson](https://twitter.com/rivershavewings).\n",
        "\n",
        "Made some small modification, tidied up things, added a neat GUI, added SLIP support (as an alternative to CLIP) and optimized the initial parameters for pixel art generation, by Philipuss#4066\n",
        "\n",
        "SLIP implementation stolen from [this notebook](https://colab.research.google.com/drive/1bItz4NdhAPHg5-u87KcH-MmJZjK-XqHN)."
      ],
      "metadata": {
        "id": "32ksbekHxZl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Citations and Licenses"
      ],
      "metadata": {
        "id": "2t_EfgIPKqc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "@misc{radford2021learning,\n",
        "      title={Learning Transferable Visual Models From Natural Language Supervision}, \n",
        "      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n",
        "      year={2021},\n",
        "      eprint={2103.00020},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CV}\n",
        "}\n",
        "\n",
        "@article{UlyanovVL17,\n",
        "    author    = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},\n",
        "    title     = {Deep Image Prior},\n",
        "    journal   = {arXiv:1711.10925},\n",
        "    year      = {2017}\n",
        "}\n",
        "\n",
        "@article{wright2021ranger21,\n",
        "      title={Ranger21: a synergistic deep learning optimizer}, \n",
        "      author={Wright, Less and Demeure, Nestor},\n",
        "      year={2021},\n",
        "      journal={arXiv preprint arXiv:2106.13731},\n",
        "}\n",
        "```\n",
        "\n",
        "### CLIP\n",
        "```\n",
        "MIT License\n",
        "Copyright (c) 2021 OpenAI\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE.\n",
        "```\n",
        "\n",
        "### Deep Image Prior\n",
        "```\n",
        "Copyright 2018 Dmitry Ulyanov\n",
        "\"Please contact me if you want to use this software in a commercial application.\" - Dmitry Ulyanov\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "```"
      ],
      "metadata": {
        "id": "lyXuhf3bK1EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import libraries"
      ],
      "metadata": {
        "id": "tbLWTWB_FmNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DmitryUlyanov/deep-image-prior\n",
        "!pip install kornia einops git+https://github.com/openai/clip madgrad\n",
        "\n",
        "!git clone https://github.com/lessw2020/Ranger21.git\n",
        "%cd Ranger21\n",
        "!python -m pip install -e .\n",
        "%cd ../\n",
        "\n",
        "!git clone https://github.com/facebookresearch/SLIP.git\n",
        "!pip install timm"
      ],
      "metadata": {
        "id": "Zrjcb_rRy_2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdOppuQOxVgR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('./deep-image-prior')\n",
        "sys.path.append('./SLIP')\n",
        "from models import *\n",
        "from utils.sr_utils import *\n",
        "import clip\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "from IPython import display\n",
        "import cv2\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import kornia.augmentation as K\n",
        "from einops import rearrange\n",
        "from madgrad import MADGRAD\n",
        "from Ranger21.ranger21.ranger21 import Ranger21\n",
        "import random\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import clear_output \n",
        "from SLIP.models import SLIP_VITB16, SLIP, SLIP_VITL16\n",
        "\n",
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View GPU details:\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "84tQr1vQy2O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87RLY8khxVgU"
      },
      "source": [
        "# Load and Configure CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzODxwinxVgV"
      },
      "outputs": [],
      "source": [
        "# CLIP works best, SLIP is kind of odd, but that might be because the settings are not optimized for it\n",
        "model_type = 'CLIP'\n",
        "\n",
        "\n",
        "if model_type == 'CLIP':\n",
        "  clip_model = clip.load('ViT-B/16', device=device)[0]\n",
        "  clip_model = clip_model.eval().requires_grad_(False)\n",
        "  clip_size = clip_model.visual.input_resolution\n",
        "elif model_type == 'SLIP':\n",
        "  model_path = \"/content/\"\n",
        "  clip_model = SLIP_VITB16(ssl_mlp_dim=4096, ssl_emb_dim=256)\n",
        "  if not os.path.exists(f'{model_path}/slip_base_100ep.pt'):\n",
        "    !wget https://dl.fbaipublicfiles.com/slip/slip_base_100ep.pt -P {model_path}\n",
        "  sd = torch.load(f'{model_path}/slip_base_100ep.pt')\n",
        "  real_sd = {}\n",
        "  for k, v in sd['state_dict'].items():\n",
        "    real_sd['.'.join(k.split('.')[1:])] = v\n",
        "  del sd\n",
        "  clip_model.load_state_dict(real_sd)\n",
        "  clip_model.requires_grad_(False).eval().to(device)\n",
        "  clip_size = 224\n",
        "\n",
        "clip_normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "def lerp(a, b, f):\n",
        "    return (a * (1.0 - f)) + (b * f);\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "\n",
        "class MakeCutoutsPhong(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = T.Compose([\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomAffine(degrees=15, translate=0.1, p=0.8, padding_mode='border', resample='bilinear'),\n",
        "            K.RandomPerspective(0.4, p=0.7, resample='bilinear'),\n",
        "            K.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1, p=0.7),\n",
        "            K.RandomGrayscale(p=0.15),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        if sideY != sideX:\n",
        "            input = K.RandomAffine(degrees=0, shear=10, p=0.5)(input)\n",
        "\n",
        "        max_size = min(sideX, sideY)\n",
        "        cutouts = []\n",
        "        for cn in range(self.cutn):\n",
        "            if cn > self.cutn - self.cutn//4:\n",
        "                cutout = input\n",
        "            else:\n",
        "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
        "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "                offsety = torch.randint(0, sideY - size + 1, ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        cutouts = torch.cat(cutouts)\n",
        "        cutouts = self.augs(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "class MakeCutoutsJuu(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            #K.RandomGaussianNoise(mean=0.0, std=0.5, p=0.1),\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "            K.RandomGrayscale(p=0.1),\n",
        "        )\n",
        "        self.noise_fac = 0.1 \n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        "\n",
        "class MakeCutoutsMoth(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs, skip_augs=False):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.skip_augs = skip_augs\n",
        "        self.augs = T.Compose([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomGrayscale(p=0.15),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            # T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "\n",
        "        cutouts = []\n",
        "        for ch in range(cutn):\n",
        "            if ch > cutn - cutn//4:\n",
        "                cutout = input.clone()\n",
        "            else:\n",
        "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
        "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
        "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "\n",
        "            if not self.skip_augs:\n",
        "                cutout = self.augs(cutout)\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "            del cutout\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        return cutouts\n",
        "\n",
        "class MakeCutoutsAaron(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = augs\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
        "          \n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "class MakeCutoutsCumin(nn.Module):\n",
        "    #from https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = nn.Sequential(\n",
        "          #K.RandomHorizontalFlip(p=0.5),\n",
        "          #K.RandomSharpness(0.3,p=0.4),\n",
        "          #K.RandomGaussianBlur((3,3),(10.5,10.5),p=0.2),\n",
        "          #K.RandomGaussianNoise(p=0.5),\n",
        "          #K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "          K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
        "          K.RandomPerspective(0.7,p=0.7),\n",
        "          K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
        "          K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),)\n",
        "            \n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "    \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "class MakeCutoutsHolywater(nn.Module):\n",
        "  def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "    super().__init__()\n",
        "    self.cut_size = cut_size\n",
        "    tqdm.write(f'cut size: {self.cut_size}')\n",
        "    self.cutn = cutn\n",
        "    self.cut_pow = cut_pow\n",
        "    self.noise_fac = 0.1\n",
        "    self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "    self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "    self.augs = nn.Sequential(\n",
        "            #K.RandomGaussianNoise(mean=0.0, std=0.5, p=0.1),\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "            K.RandomGrayscale(p=0.1),\n",
        "        )\n",
        "\n",
        "  def set_cut_pow(self, cut_pow):\n",
        "    self.cut_pow = cut_pow\n",
        "\n",
        "  def forward(self, input):\n",
        "      sideY, sideX = input.shape[2:4]\n",
        "      max_size = min(sideX, sideY)\n",
        "      min_size = min(sideX, sideY, self.cut_size)\n",
        "      cutouts = []\n",
        "      cutouts_full = []\n",
        "      noise_fac = 0.1\n",
        "      min_size_width = min(sideX, sideY)\n",
        "      lower_bound = float(self.cut_size/min_size_width)\n",
        "      \n",
        "      for ii in range(self.cutn):\n",
        "        size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "        randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "        size_mult = randsize ** self.cut_pow * ii + size\n",
        "        size1 = int((min_size_width) * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "        size2 = int((min_size_width) * torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "        offsetx = torch.randint(0, sideX - size1 + 1, ())\n",
        "        offsety = torch.randint(0, sideY - size2 + 1, ())\n",
        "        cutout = input[:, :, offsety:offsety + size2 + ii, offsetx:offsetx + size1 + ii]\n",
        "        cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "      \n",
        "      cutouts = torch.cat(cutouts, dim=0)\n",
        "      cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "      cutouts = self.augs(cutouts)\n",
        "      facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "      cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "      return cutouts\n",
        "\n",
        "class MakeCutoutsOldHolywater(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = nn.Sequential(\n",
        "          #K.RandomHorizontalFlip(p=0.5),\n",
        "          #K.RandomSharpness(0.3,p=0.4),\n",
        "          #K.RandomGaussianBlur((3,3),(10.5,10.5),p=0.2),\n",
        "          #K.RandomGaussianNoise(p=0.5),\n",
        "          #K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "          K.RandomAffine(degrees=180, translate=0.5, p=0.2, padding_mode='border'),\n",
        "          K.RandomPerspective(0.6,p=0.9),\n",
        "          K.ColorJitter(hue=0.03, saturation=0.01, p=0.1),\n",
        "          K.RandomErasing((.1, .7), (.3, 1/.4), same_on_batch=True, p=0.2),)\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "    \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "class MakeCutoutsGinger(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = augs\n",
        "        '''\n",
        "        nn.Sequential(\n",
        "          K.RandomHorizontalFlip(p=0.5),\n",
        "          K.RandomSharpness(0.3,p=0.4),\n",
        "          K.RandomGaussianBlur((3,3),(10.5,10.5),p=0.2),\n",
        "          K.RandomGaussianNoise(p=0.5),\n",
        "          K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "          K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "          K.RandomPerspective(0.2,p=0.4, ),\n",
        "          K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),)\n",
        "'''\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "class MakeCutoutsZynth(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = nn.Sequential(\n",
        "        K.RandomHorizontalFlip(p=0.5),\n",
        "        # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "        K.RandomSharpness(0.3,p=0.4),\n",
        "        K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "        K.RandomPerspective(0.2,p=0.4),\n",
        "        K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "class MakeCutoutsWyvern(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = augs\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        return clamp_with_grad(torch.cat(cutouts, dim=0), 0, 1)\n",
        "\n",
        "flavors = {\n",
        "    \"phong\": MakeCutoutsPhong,\n",
        "    \"cumin\": MakeCutoutsCumin,\n",
        "    \"holywater\": MakeCutoutsHolywater,\n",
        "    \"old_holywater\": MakeCutoutsOldHolywater,\n",
        "    \"ginger\": MakeCutoutsGinger,\n",
        "    \"zynth\": MakeCutoutsZynth,\n",
        "    \"wyvern\": MakeCutoutsWyvern,\n",
        "    \"aaron\": MakeCutoutsAaron,\n",
        "    \"moth\": MakeCutoutsMoth,\n",
        "    \"juu\": MakeCutoutsJuu,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization loop"
      ],
      "metadata": {
        "id": "1_39CKmTIR68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWL7PGGtxVgW"
      },
      "outputs": [],
      "source": [
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def optimize_network(num_iterations, optimizer_type, lr, flavor, cutn, cut_pow, upsample_mode):\n",
        "    global itt\n",
        "    itt = 0\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        random.seed(seed)\n",
        "    \n",
        "    augs = None\n",
        "    make_cutouts = flavors[flavor](clip_size, cutn, cut_pow, augs)\n",
        "\n",
        "    # Initialize DIP skip network\n",
        "    input_depth = 32\n",
        "    net = get_net(\n",
        "        input_depth, 'skip',\n",
        "        pad='reflection',\n",
        "        skip_n33d=128, skip_n33u=128,\n",
        "        skip_n11=4, num_scales=5,\n",
        "        upsample_mode=upsample_mode,\n",
        "        # stride | avg | max | lanczos2\n",
        "        # Stride, Avg and Max are pretty similar\n",
        "        # Completely removing downsample_mode or using lanczos2 works best\n",
        "        downsample_mode='lanczos2',\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize input noise\n",
        "    net_input = torch.zeros([1, input_depth, sideY, sideX], device=device).normal_().div(10).detach()\n",
        "\n",
        "    # Encode text prompt with CLIP\n",
        "    target_embed = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
        "\n",
        "    if optimizer_type == 'Ranger21':\n",
        "      optimizer = Ranger21(net.parameters(), lr, weight_decay=0.01, num_epochs=200, num_batches_per_epoch=1)\n",
        "    elif optimizer_type == 'Adam':\n",
        "      optimizer = torch.optim.Adam(net.parameters(), lr)\n",
        "    elif optimizer_type == 'MadGrad':\n",
        "      optimizer = MADGRAD(net.parameters(), lr, weight_decay=0.01, momentum=0.9)\n",
        "        \n",
        "    try:\n",
        "        for _ in range(num_iterations):\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "    \n",
        "            out = net(net_input)\n",
        "            cutouts = make_cutouts(out)\n",
        "            image_embeds = clip_model.encode_image(clip_normalize(cutouts))\n",
        "            loss = spherical_dist_loss(image_embeds, target_embed).mean()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            itt += 1\n",
        "            save_progress_video = False\n",
        "\n",
        "            if itt % display_rate == 0 or save_progress_video:\n",
        "                with torch.inference_mode():\n",
        "                    image = TF.to_pil_image(out[0].clamp(0, 1))\n",
        "                    if itt % display_rate == 0:\n",
        "                        display.clear_output(wait=True)\n",
        "                        display.display(image)\n",
        "                        if display_augs:\n",
        "                            aug_grid = torchvision.utils.make_grid(cutouts, nrow=math.ceil(math.sqrt(cutn)))\n",
        "                            display.display(TF.to_pil_image(aug_grid.clamp(0, 1)))\n",
        "                    if save_progress_video and itt > 15:\n",
        "                        video_writer.append_data(np.asarray(image))\n",
        "\n",
        "            if anneal_lr:\n",
        "                optimizer.param_groups[0]['lr'] = max(0.00001, .99 * optimizer.param_groups[0]['lr'])\n",
        "\n",
        "            print(f'Iteration:  {itt} / {num_iterations}')\n",
        "    \n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        return TF.to_pil_image(net(net_input)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pmx90a2xVgX"
      },
      "source": [
        "# Settings / Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KStRsOWgxVga",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Configure and Run**\n",
        "#@markdown Make sure to add `#pixelart` at the end of your prompt\n",
        "prompt = 'A slime monster. #pixelart' #@param{type:'string'}\n",
        "aspect = \"widescreen\" #@param [\"widescreen\", \"square\"]\n",
        "#@markdown seed 0 will generate a random seed every time\n",
        "seed =  0#@param{type:'number'}\n",
        "num_iterations = 250 #@param{type:'number'}\n",
        "#@markdown Number of crops of image shown to CLIP, this can affect quality\n",
        "cutn = 60 #@param{type:'number'}\n",
        "#@markdown Values of cut_pow below 1 prioritize structure over detail, and vice versa for above 1\n",
        "cut_pow = 0.85 #@param{type:'number'}\n",
        "step_size = 0.0025 #@param{type:'number'}\n",
        "display_rate = 50 #@param{type:'number'}\n",
        "\n",
        "if aspect == 'widescreen':\n",
        "  sideX = 640\n",
        "  sideY = 384\n",
        "  k = 5\n",
        "elif aspect == 'square':\n",
        "  sideX = 512\n",
        "  sideY = 512\n",
        "  k = 6\n",
        "if seed == 0:\n",
        "  seed = random.randint(0, 2**32)\n",
        "  print(\"Seed: \" + str(seed))\n",
        "anneal_lr = True # True == lower the learning rate over time\n",
        "lr = step_size\n",
        "\n",
        "# Adam is pretty bad for pixelart\n",
        "# MagGrad is a bit better\n",
        "# Ranger21 is by far the best\n",
        "opt_type = 'Ranger21' # Adam, MadGrad, Ranger21\n",
        "\n",
        "# Most flavors give a neat effect, but not really pixelart-y\n",
        "# Cumin is the best of them all, though moth is kind of neat as well\n",
        "# All flavors are imported from hypertron V2\n",
        "flavor = 'cumin' #[\"phong\", \"ginger\", \"cumin\", \"holywater\", \"zynth\", \"wyvern\", \"aaron\", \"moth\", \"juu\"]\n",
        "\n",
        "# Nearest is the best for pixel art\n",
        "# Bicubic and bilinear are mostly the same, they make organic and smooth shapes\n",
        "scaling_mode = \"nearest\" # nearest | bilinear | bicubic\n",
        "display_augs = False # Display grid of augmented image, for debugging\n",
        "\n",
        "# Begin optimization / generation\n",
        "out = optimize_network(num_iterations, opt_type, lr, flavor, cutn, cut_pow, scaling_mode)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "out.save(f'progress.png', quality=100)\n",
        "input = cv2.imread('progress.png')\n",
        "height, width = input.shape[:2]\n",
        "w, h = (int(sideX/k), int(sideY/k))\n",
        "temp = cv2.resize(input, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "output = cv2.resize(temp, (width, height), interpolation=cv2.INTER_NEAREST)\n",
        "cv2_imshow(output)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "CLIP+DIP for pixel art",
      "provenance": [],
      "collapsed_sections": [
        "2t_EfgIPKqc9",
        "tbLWTWB_FmNA",
        "87RLY8khxVgU",
        "1_39CKmTIR68"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}