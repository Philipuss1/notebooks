{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "🌋 Hypertron v1 🌋",
      "provenance": [],
      "collapsed_sections": [
        "f8BZHJvLHzqz",
        "h9VpgIGi_utC"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "# <font size=\"+3\">🌋</font> <font size=\"+3\">**Hypertron v1**</font> <font size=\"+3\">🌋</font>\n",
        "by Philipuss#4066\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Disclaimer:\n",
        "I, Philipuss, do not claim owenership of the whole notebook, only on the changes I made. I don't want to look like the one who created or invented this. Please read the credits bellow: \n",
        "\n",
        "Originally made by [Katherine Crowson](https://github.com/crowsonkb) - [@RiversHaveWings](https://twitter.com/RiversHaveWings). The original BigGAN+CLIP method was by [@advadnoun](https://twitter.com/advadnoun).\n",
        "\n",
        "Added some explanations and modifications by Eleiber#8347, and the (previous) GUI was made with the help of Abulafia#3734.\n",
        "\n",
        "Streamlined UI, added metadata tagging, and added zip/transfer to drive by pseu#3017.\n",
        "\n",
        "Added QOL functions for making batches of images and saving outputs to Google Drive, as well as incorporating MSE Regulization and EMA Tensor, made more parameters configurable and added altprompts. - Varkarrus.\n",
        "\n",
        "The MSE Regulization and EMA Tensor are taken from [this notebook](https://colab.research.google.com/drive/1gFn9u3oPOgsNzJWEFmdK-N9h_y65b8fj?usp=sharing) by [@jbusted1](https://twitter.com/jbusted1).\n",
        "\n",
        "---\n",
        "Help:\n",
        "\n",
        "More info on flavors [here](https://i.ibb.co/vBszFNJ/Flavors.png).\n",
        "\n",
        "More info on prompt experiments [here](https://i.ibb.co/0FF7vNn/prompt-experiments.png).\n",
        "\n",
        "\n",
        "The styles of made-up, not real, artists can be found [here](https://docs.google.com/spreadsheets/d/1nMq-TjBj3t6us-npLRoLFq0VtgpVwdXCKTcQgnxKgTQ/edit?usp=sharing).\n",
        "\n",
        "Keywords cheatsheet can be found [here](https://imgur.com/a/SnSIQRu) (made by kingdomakrillic).\n",
        "\n",
        "A short guide to prompt engineering can be found [here](https://docs.google.com/document/d/1qy5fdeThu7pIikulQuWpmKvYBiv9wMshIHcrBr-VldA/edit?usp=sharing).\n",
        "\n",
        "---\n",
        "\n",
        "## Changelog\n",
        "#### **v1:**\n",
        "\n",
        "Added prompt builder, prompt speed estimator, time-out preventor, tidier GUI, random init image generator and implemented flavors, as well as custom flavor maker and importer, implemented pixel art, updated old super-resolution utility, added new download mirrors for the models, added transparent png support, fixed a few Colab bugs, added templates, added CLIPIT compatibility, added super slowomo, added optimizer changing during the MSE phase to improve quality, made the CLIP model a configurable parameter, added prompt experiments, added different types of init images (different noises, etc.), fixed prompt weights and image prompts (target images), small QoL things all over the place, made the notebook compatible with Gumbel F8 and Sber's Gumbel model (better at faces and text).\n",
        "\n",
        "Credits:\n",
        "\n",
        "Main flavor implementation taken from [this notebook](https://colab.research.google.com/drive/1n_xrgKDlGQcCF6O-eL3NOd_x4NSqAUjK) by Hillel Wayne  ([website](https://www.hillelwayne.com/), [twitter](https://twitter.com/hillelogram))\n",
        "\n",
        "Pixel art taken from [this notebook](https://colab.research.google.com/github/dribnet/clipit/blob/master/demos/PixelDrawer.ipynb) by [@dribnet](https://twitter.com/dribnet) (who also made CLIPIT).\n",
        "\n",
        "CLIPIT configuration script made by Varkarrus#9927 and gotten from [this notebook](https://colab.research.google.com/drive/1RwbAZogx83H0FM_MXiSDnbfIM49DFtcA).\n",
        "\n",
        "Zynth flavor is an implementation of [this notebook](https://colab.research.google.com/github/justinjohn0306/VQGAN-CLIP/blob/main/VQGAN%2BCLIP_(Zooming)_(z%2Bquantize_method_with_addons).ipynb) by [Chigozie Nri](https://twitter.com/chigozienri).\n",
        "\n",
        "Wyvern flavor is an implementation of [this notebook](https://colab.research.google.com/gist/afiaka87/a97cca3b54c02209b94ff805224f9eb5/zeta_quantize.ipynb).\n",
        "\n",
        "Super slowmo utility from [this notebook](https://colab.research.google.com/drive/10Sm03OcXzsb3mApzvaZhEA1SxkVx1aio) by [Chigozie Nri](https://twitter.com/chigozienri).\n",
        "\n",
        "Optimizer changing inspired from [this notebook](https://colab.research.google.com/drive/1MgIWhmcjD846xhCBfDMaI9hx02XqsLzi) by Varkarrus.\n",
        "\n",
        "Added a few small features to prompt weights that I got from [this notebook](https://colab.research.google.com/github/sadnow/360Diffusion/blob/main/360Diffusion_Public.ipynb) by sadnow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nnPDoDFCp6n",
        "cellView": "form"
      },
      "source": [
        "# @title <font size=\"+2\">📊</font> License & Check GPU <font size=\"+2\">📊</font>\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE.\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8BZHJvLHzqz"
      },
      "source": [
        "### <font size=\"+2\">👅</font> Flavor Maker & Reader <font size=\"+2\">👅</font>\n",
        "Not important, ignore this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "xlu96HUory6l"
      },
      "source": [
        "import os\n",
        "\n",
        "#@title Flavor Maker\n",
        "#@markdown Tweak the values and run this cell. Then choose \"custom\" flavor when running the generation cell. \n",
        "\n",
        "Flavor_Name = \"Flavor\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "Random_Horizontal_Flip = 0.5 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "Random_Sharpness = 0.3 #@param {type:\"number\"}\n",
        "Random_Sharpness_P = 0.4 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "Random_Gaussian_Blur = 3.3 #@param {type:\"number\"}\n",
        "Random_Gaussian_Blur_W = 10.5 #@param {type:\"number\"}\n",
        "Random_Gaussian_Blur_H = 10.5 #@param {type:\"number\"}\n",
        "Random_Gaussian_Blur_P = 0.2 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "Random_Gaussian_Noise_P = 0.5 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "Random_Elastic_Transform_Kernel_Size_W = 33 #@param {type:\"number\"}\n",
        "Random_Elastic_Transform_Kernel_Size_H = 33 #@param {type:\"number\"}\n",
        "Random_Elastic_Transform_Sigma = 7.7 #@param {type:\"number\"}\n",
        "Random_Elastic_Transform_P = 0.2 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "Random_Affine_Degrees = 180 #@param {type:\"number\"}\n",
        "Random_Affine_Translate = 0.5 #@param {type:\"number\"}\n",
        "Random_Affine_P = 0.2 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "Random_Perspective = 0.6 #@param {type:\"number\"}\n",
        "Random_Perspective_P = 0.9 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "Color_Jitter_Hue = 0.03 #@param {type:\"number\"}\n",
        "Color_Jitter_Saturation = 0.01 #@param {type:\"number\"}\n",
        "Color_Jitter_P = 0.1 #@param {type:\"number\"}\n",
        "##@markdown ---\n",
        "#Random_Erasing = 0.5 #@param {type:\"number\"}\n",
        "\n",
        "Flavor_File = str(Random_Horizontal_Flip) + \",\\n\" + str(Random_Sharpness) + \",\\n\" + str(Random_Sharpness_P) + \",\\n\" +  str(Random_Gaussian_Blur) + \",\\n\" + str(Random_Gaussian_Blur_W) + \",\\n\" +  str(Random_Gaussian_Blur_H) + \",\\n\" + str(Random_Gaussian_Blur_P) + \",\\n\" + str(Random_Gaussian_Noise_P) + \",\\n\" + str(Random_Elastic_Transform_Kernel_Size_W) + \",\\n\" + str(Random_Elastic_Transform_Kernel_Size_H) + \",\\n\" + str(Random_Elastic_Transform_Sigma) + \",\\n\" + str(Random_Elastic_Transform_P) + \",\\n\" + str(Random_Affine_Degrees) + \",\\n\" + str(Random_Affine_Translate) + \",\\n\" + str(Random_Affine_P) + \",\\n\" + str(Random_Perspective) + \",\\n\" + str(Random_Perspective_P) + \",\\n\" + str(Color_Jitter_Hue) + \",\\n\" + str(Color_Jitter_Saturation) + \",\\n\" + str(Color_Jitter_P) + \",,\"\n",
        " \n",
        "#print(Flavor_File)\n",
        "Save_File = os.path.join(\"/content/\", Flavor_Name + \".flavor\")\n",
        "\n",
        "file1 = open(Save_File, \"w\")\n",
        "file1. write(Flavor_File)\n",
        "file1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "MzTKoZzO1cBZ"
      },
      "source": [
        "#@title Flavor Importer\n",
        "#@markdown Upload .flavor file to Colab, then enter the name and run this cell. Select the \"custom\" flavor when running.\n",
        "\n",
        "Flavor_File_Name = \"Flavor.flavor\" #@param {type:\"string\"}\n",
        "\n",
        "a_file = open(Flavor_File_Name)\n",
        "\n",
        "def read_flavor_line (line_num): \n",
        "  with open(Flavor_File_Name) as fp:\n",
        "    for i, line in enumerate(fp):\n",
        "        if i == line_num:\n",
        "          line = line[:-2]\n",
        "          #print(line)\n",
        "          return line\n",
        "  #lines = str(a_file.readlines(line_num))\n",
        "  #lines = lines.replace(\"['\", '')\n",
        "  #lines = lines.replace(\",\", \"\")\n",
        "  #lines = lines.replace(\"n']\", \"\")\n",
        "  #print(lines)\n",
        "\n",
        "##@markdown ---\n",
        "Random_Horizontal_Flip = read_flavor_line(1)\n",
        "##@markdown ---\n",
        "Random_Sharpness = read_flavor_line(2) \n",
        "Random_Sharpness_P = read_flavor_line(3)\n",
        "##@markdown ---\n",
        "Random_Gaussian_Blur = read_flavor_line(4)\n",
        "Random_Gaussian_Blur_W = read_flavor_line(5)\n",
        "Random_Gaussian_Blur_H = read_flavor_line(6)\n",
        "Random_Gaussian_Blur_P = read_flavor_line(7)\n",
        "Random_Gaussian_Noise_P = read_flavor_line(8)\n",
        "##@markdown ---\n",
        "Random_Elastic_Transform_Kernel_Size_W = read_flavor_line(9)\n",
        "Random_Elastic_Transform_Kernel_Size_H = read_flavor_line(10)\n",
        "Random_Elastic_Transform_Sigma = read_flavor_line(11)\n",
        "Random_Elastic_Transform_P = read_flavor_line(12)\n",
        "##@markdown ---\n",
        "Random_Affine_Degrees = read_flavor_line(13)\n",
        "Random_Affine_Translate = read_flavor_line(14)\n",
        "Random_Affine_P = read_flavor_line(15)\n",
        "##@markdown ---\n",
        "Random_Perspective = read_flavor_line(16)\n",
        "Random_Perspective_P = read_flavor_line(17)\n",
        "##@markdown ---\n",
        "Color_Jitter_Hue = read_flavor_line(18)\n",
        "Color_Jitter_Saturation = read_flavor_line(19)\n",
        "Color_Jitter_P = read_flavor_line(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIsqixOqPu8D"
      },
      "source": [
        "### Download and set things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA1PHoJrRiK9",
        "cellView": "form"
      },
      "source": [
        "#@title <font color=\"lightgreen\" size=\"+3\">←</font> <font size=\"+2\">🎡</font> Installing and initializing libraries **[RUN ME]** <font size=\"+2\">🎡</font>\n",
        "#@markdown This cell could take a while since it downloads several libraries. Select CLIPIT if you plan to generate pixel art as well or to use CLIPIT. \"Video\" if you plan on creating a video. Don't download any if you don't **need** them.\n",
        "#@markdown If you have to restart the notebook and you have already downloaded everything, uncheck `Main_Libraries`.\n",
        "\n",
        "#@markdown After running this cell, restart the runtime and run it again. Silly Colab. \n",
        "\n",
        "#Woohoo, getting technical and looking behind the curtains, huh? Well, I have a little game for you.\n",
        "#I've hidden Hamilton references all over the place, go find them and if you don't know Hamilton,\n",
        "#go watch Hamilton and if you don't, that's bad.\n",
        "\n",
        "Main_Libraries = True #@param {type:\"boolean\"}\n",
        "Download_CLIPIT = False #@param {type:\"boolean\"}\n",
        "Download_Video = False #@param {type:\"boolean\"}\n",
        "Download_Super_Res = False #@param {type:\"boolean\"}\n",
        "Download_Super_Slomo = False #@param {type:\"boolean\"}\n",
        "\n",
        "if Main_Libraries == True:\n",
        "  print('GPU:')\n",
        "  !nvidia-smi --query-gpu=name,memory.total --format=csv\n",
        "\n",
        "  print(\"\\nDownloading CLIP...\")\n",
        "  !git clone https://github.com/openai/CLIP                  &> /dev/null\n",
        "  \n",
        "  print(\"Installing AI Python libraries...\")\n",
        "  !git clone https://github.com/CompVis/taming-transformers  &> /dev/null\n",
        "  !pip install ftfy regex tqdm omegaconf pytorch-lightning   &> /dev/null\n",
        "  !pip install kornia                                        &> /dev/null\n",
        "  !pip install einops                                        &> /dev/null\n",
        "  !pip install transformers                                  &> /dev/null\n",
        "  !pip install torch_optimizer                               &> /dev/null\n",
        "\n",
        "  !pip install noise                                         &> /dev/null\n",
        "  \n",
        "  #!git clone https://github.com/lessw2020/Ranger21.git       &> /dev/null\n",
        "  #!cd Ranger21                                               &> /dev/null\n",
        "  #!pip install -e .                                          &> /dev/null\n",
        "  #!cd ..                                                     &> /dev/null\n",
        "\n",
        "  !mkdir steps\n",
        "  %mkdir Init_Img\n",
        "\n",
        "  print(\"Installing libraries for handling metadata...\")\n",
        "  !pip install stegano                                       &> /dev/null\n",
        "  !apt install exempi                                        &> /dev/null\n",
        "  !pip install python-xmp-toolkit                            &> /dev/null\n",
        "  !pip install imgtag                                        &> /dev/null\n",
        "\n",
        "  if Download_Video:\n",
        "    print(\"Installing Python libraries for video creation...\")\n",
        "    !pip install imageio-ffmpeg &> /dev/null\n",
        "\n",
        "  if Download_Super_Res:\n",
        "    print(\"Installing Python libraries for super resolution...\")\n",
        "    # Clone Real-ESRGAN and enter the Real-ESRGAN\n",
        "    !git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "    %cd Real-ESRGAN\n",
        "    # Set up the environment\n",
        "    !pip install basicsr\n",
        "    !pip install facexlib\n",
        "    !pip install gfpgan\n",
        "    !pip install -r requirements.txt\n",
        "    !python setup.py develop\n",
        "    # Download the pre-trained model\n",
        "    !wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P experiments/pretrained_models\n",
        "    \n",
        "\n",
        "  if Download_Super_Slomo:\n",
        "    !git clone -q --depth 1 https://github.com/avinashpaliwal/Super-SloMo.git\n",
        "    from os.path import exists\n",
        "    def download_from_google_drive(file_id, file_name):\n",
        "      # download a file from the Google Drive link\n",
        "      !rm -f ./cookie\n",
        "      !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "      confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "      confirm_text = confirm_text[0]\n",
        "      !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "      \n",
        "    pretrained_model = 'SuperSloMo.ckpt'\n",
        "    if not exists(pretrained_model):\n",
        "      download_from_google_drive('1IvobLDbRiBgZr3ryCRrWL8xDbMZ-KnpF', pretrained_model)\n",
        "\n",
        "  %mkdir png_processing\n",
        "\n",
        "  %mkdir templates\n",
        "  !curl https://i.ibb.co/3kn9Qrv/flag.png -o templates/flag.png &> /dev/null\n",
        "  !curl https://i.ibb.co/0BHqVyg/14135136623-3973d3f03c-z.jpg -o templates/planet.png &> /dev/null\n",
        "  !curl https://i.ibb.co/52WMK2M/j7oocvu80qe11.png -o templates/map.png &> /dev/null\n",
        "  !curl https://i.ibb.co/3fg9Zkx/creature.png -o templates/creature.png &> /dev/null\n",
        "  !curl https://i.ibb.co/X3Mh2pP/human.jpg -o templates/human.png &> /dev/null\n",
        "\n",
        "\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "#from Ranger21.ranger21 import ranger21\n",
        "\n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import hashlib\n",
        "from PIL.PngImagePlugin import PngImageFile, PngInfo\n",
        "import json\n",
        "\n",
        "import IPython\n",
        "from IPython.display import Markdown, display, Image, clear_output\n",
        "import urllib.request\n",
        "import random\n",
        "from random import randint\n",
        "\n",
        "from imgtag import ImgTag    # metadata \n",
        "from libxmp import *         # metadata\n",
        "import libxmp                # metadata\n",
        "from stegano import lsb\n",
        "\n",
        "if Download_CLIPIT:\n",
        "  print(\"Installing CLIPIT...\")\n",
        "  from IPython.utils import io\n",
        "  with io.capture_output() as captured:\n",
        "    #!git clone https://github.com/openai/CLIP\n",
        "    # !pip install taming-transformers\n",
        "    #!git clone https://github.com/CompVis/taming-transformers.git\n",
        "    !rm -Rf clipit\n",
        "    !git clone -b future https://github.com/Philipuss1/clipit CLIPIT &> /dev/null\n",
        "    #!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "    #!pip install kornia\n",
        "    #!pip install imageio-ffmpeg   \n",
        "    #!pip install einops\n",
        "    !pip install torch-optimizer &> /dev/null\n",
        "    !pip install easydict &> /dev/null\n",
        "    !pip install braceexpand &> /dev/null\n",
        "    !pip install git+https://github.com/pvigier/perlin-numpy &> /dev/null\n",
        "\n",
        "    # ClipDraw deps\n",
        "    !pip install svgwrite\n",
        "    !pip install svgpathtools\n",
        "    !pip install cssutils\n",
        "    !pip install numba\n",
        "    !pip install torch-tools\n",
        "    !pip install visdom\n",
        "\n",
        "    !git clone https://github.com/BachiLi/diffvg &> /dev/null\n",
        "    %cd diffvg\n",
        "    # !ls\n",
        "    !git submodule update --init --recursive\n",
        "    !python setup.py install\n",
        "    %cd ..\n",
        "\n",
        "  sys.path.append(\"clipit\")\n",
        "\n",
        "  result_msg = \"setup complete\"\n",
        " \n",
        "  import IPython\n",
        "  import os\n",
        "  if not os.path.isfile(\"first_init_complete\"):\n",
        "  # put stuff in here that should only happen once\n",
        "    !mkdir -p models\n",
        "    os.mknod(\"first_init_complete\")\n",
        "    result_msg = \"Please choose Runtime -> Restart Runtime from the menu, and then run Setup again\"\n",
        "\n",
        "  js_code = f'''\n",
        "  document.querySelector(\"#output-area\").appendChild(document.createTextNode(\"{result_msg}\"));\n",
        "  '''\n",
        "  js_code += '''\n",
        "  for (rule of document.styleSheets[0].cssRules){\n",
        "    if (rule.selectorText=='body') break\n",
        "  }\n",
        "  rule.style.fontSize = '30px'\n",
        "  '''\n",
        "  display(IPython.display.Javascript(js_code))\n",
        "\n",
        "  print(\"Installed pixel art libraries\")\n",
        "  ###################################\n",
        "\n",
        "print(\"Finished!\")\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "def lerp(a, b, f):\n",
        "    return (a * (1.0 - f)) + (b * f);\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "#You want to read something spicy? Read The Reynolds Pamphlet and cringe at Hamilton ruining his life.\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "#def parse_prompt(prompt):\n",
        "#    vals = prompt.rsplit(':', 2)\n",
        "#    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "#    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "\n",
        "\n",
        "############################################################################################\n",
        "############################################################################################\n",
        "\n",
        "\n",
        "class MakeCutoutsCustom(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = nn.Sequential(\n",
        "          K.RandomHorizontalFlip(p=Random_Horizontal_Flip),\n",
        "          K.RandomSharpness(Random_Sharpness,p=Random_Sharpness_P),\n",
        "          K.RandomGaussianBlur((Random_Gaussian_Blur),(Random_Gaussian_Blur_W,Random_Gaussian_Blur_W),p=Random_Gaussian_Blur_P),\n",
        "          K.RandomGaussianNoise(p=Random_Gaussian_Noise_P),\n",
        "          K.RandomElasticTransform(kernel_size=(Random_Elastic_Transform_Kernel_Size_W, Random_Elastic_Transform_Kernel_Size_H), sigma=(Random_Elastic_Transform_Sigma), p=Random_Elastic_Transform_P),\n",
        "          K.RandomAffine(degrees=Random_Affine_Degrees, translate=Random_Affine_Translate, p=Random_Affine_P, padding_mode='border'),\n",
        "          K.RandomPerspective(Random_Perspective,p=Random_Perspective_P),\n",
        "          K.ColorJitter(hue=Color_Jitter_Hue, saturation=Color_Jitter_Saturation, p=Color_Jitter_P),)\n",
        "          #K.RandomErasing((0.1, 0.7), (0.3, 1/0.4), same_on_batch=True, p=0.2),)\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "    \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "class MakeCutoutsCumin(nn.Module):\n",
        "    #from https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = nn.Sequential(\n",
        "          #K.RandomHorizontalFlip(p=0.5),\n",
        "          #K.RandomSharpness(0.3,p=0.4),\n",
        "          #K.RandomGaussianBlur((3,3),(10.5,10.5),p=0.2),\n",
        "          #K.RandomGaussianNoise(p=0.5),\n",
        "          #K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "          K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n",
        "          K.RandomPerspective(0.7,p=0.7),\n",
        "          K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n",
        "          K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),)\n",
        "            \n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "    \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "class MakeCutoutsHolywater(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = nn.Sequential(\n",
        "          #K.RandomHorizontalFlip(p=0.5),\n",
        "          #K.RandomSharpness(0.3,p=0.4),\n",
        "          #K.RandomGaussianBlur((3,3),(10.5,10.5),p=0.2),\n",
        "          #K.RandomGaussianNoise(p=0.5),\n",
        "          #K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "          K.RandomAffine(degrees=180, translate=0.5, p=0.2, padding_mode='border'),\n",
        "          K.RandomPerspective(0.6,p=0.9),\n",
        "          K.ColorJitter(hue=0.03, saturation=0.01, p=0.1),\n",
        "          K.RandomErasing((.1, .7), (.3, 1/.4), same_on_batch=True, p=0.2),)\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "    \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "class MakeCutoutsGinger(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = augs\n",
        "        '''\n",
        "        nn.Sequential(\n",
        "          K.RandomHorizontalFlip(p=0.5),\n",
        "          K.RandomSharpness(0.3,p=0.4),\n",
        "          K.RandomGaussianBlur((3,3),(10.5,10.5),p=0.2),\n",
        "          K.RandomGaussianNoise(p=0.5),\n",
        "          K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "          K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "          K.RandomPerspective(0.2,p=0.4, ),\n",
        "          K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),)\n",
        "'''\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "class MakeCutoutsZynth(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = nn.Sequential(\n",
        "        K.RandomHorizontalFlip(p=0.5),\n",
        "        # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "        K.RandomSharpness(0.3,p=0.4),\n",
        "        K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "        K.RandomPerspective(0.2,p=0.4),\n",
        "        K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        noise_fac = 0.1\n",
        "        \n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          randsize = torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound,1.)\n",
        "          size_mult = randsize ** self.cut_pow\n",
        "          size = int(min_size_width * (size_mult.clip(lower_bound, 1.))) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        \n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        cutouts = clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "        #if args.use_augs:\n",
        "        cutouts = self.augs(cutouts)\n",
        "        if self.noise_fac:\n",
        "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        return cutouts\n",
        "\n",
        "class MakeCutoutsWyvern(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow, augs):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        tqdm.write(f'cut size: {self.cut_size}')\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.noise_fac = 0.1\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "        self.augs = augs\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        return clamp_with_grad(torch.cat(cutouts, dim=0), 0, 1)\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        print(config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, PIL.Image.LANCZOS)\n",
        "\n",
        "BUF_SIZE = 65536\n",
        "def get_digest(path, alg=hashlib.sha256):\n",
        "  hash = alg()\n",
        "  print(path)\n",
        "  with open(path, 'rb') as fp:\n",
        "    while True:\n",
        "      data = fp.read(BUF_SIZE)\n",
        "      if not data: break\n",
        "      hash.update(data)\n",
        "  return b64encode(hash.digest()).decode('utf-8')\n",
        "\n",
        "flavordict = {\n",
        "    \"cumin\": MakeCutoutsCumin,\n",
        "    \"holywater\": MakeCutoutsHolywater,\n",
        "    \"ginger\": MakeCutoutsGinger,\n",
        "    \"zynth\": MakeCutoutsZynth,\n",
        "    \"wyvern\": MakeCutoutsWyvern,\n",
        "    \"custom\": MakeCutoutsCustom\n",
        "}\n",
        "\n",
        "@torch.jit.script\n",
        "def gelu_impl(x):\n",
        "    \"\"\"OpenAI's gelu implementation.\"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x * (1.0 + 0.044715 * x * x)))\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    return gelu_impl(x)\n",
        "\n",
        "\n",
        "class ModelHost:\n",
        "  def __init__(self, args):\n",
        "    self.args = args\n",
        "    self.model, self.perceptor = None, None\n",
        "    self.make_cutouts = None\n",
        "    self.alt_make_cutouts = None\n",
        "    self.imageSize = None\n",
        "    self.prompts = None\n",
        "    self.opt = None\n",
        "    self.normalize = None\n",
        "    self.z, self.z_orig, self.z_min, self.z_max = None, None, None, None\n",
        "    self.metadata = None\n",
        "    self.mse_weight = 0\n",
        "    self.normal_flip_optim = None\n",
        "    self.usealtprompts = False\n",
        "\n",
        "  def setup_metadata(self, seed):\n",
        "    metadata = {k:v for k,v in vars(self.args).items()}\n",
        "    del metadata['max_iterations']\n",
        "    del metadata['display_freq']\n",
        "    metadata['seed'] = seed\n",
        "    if (metadata['init_image']):\n",
        "      path = metadata['init_image']\n",
        "      digest = get_digest(path)\n",
        "      metadata['init_image'] = (path, digest)\n",
        "    if (metadata['image_prompts']):\n",
        "      prompts = []\n",
        "      for prompt in metadata['image_prompts']:\n",
        "        path = prompt\n",
        "        digest = get_digest(path)\n",
        "        prompts.append((path,digest))\n",
        "      metadata['image_prompts'] = prompts\n",
        "    self.metadata = metadata\n",
        "\n",
        "  '''\n",
        "  How does a ragtag volunteer army in need of a shower somehow defeat a global superpower?\n",
        "  How do we emerge victorious from the quagmire? Leave the battlefield waving Betsy Ross' flag higher?\n",
        "  Yo, turns out we have a secret weapon, an immigrant, you know and love, who's unafraid to step in.\n",
        "  He's constantly confusin', confoundin' the British henchmen, everyone give it up for America's favorite fighting Frenchman!\n",
        "  **L a f a y e t t e!**\n",
        "  '''\n",
        "\n",
        "  def setup_model(self, x):\n",
        "    i = x\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Using device:', device)\n",
        "    if self.args.prompts:\n",
        "        print('Using prompts:', self.args.prompts)\n",
        "    if self.args.altprompts:\n",
        "        print('Using alternate augment set prompts:', self.args.altprompts)\n",
        "    if self.args.image_prompts:\n",
        "        print('Using image prompts:', self.args.image_prompts)\n",
        "    if args.seed is None:\n",
        "        seed = torch.seed()\n",
        "    else:\n",
        "        seed = args.seed\n",
        "    torch.manual_seed(seed)\n",
        "    print('Using seed:', seed)\n",
        "\n",
        "    model = load_vqgan_model(f'{args.vqgan_model}.yaml', f'{args.vqgan_model}.ckpt').to(device)\n",
        "    \n",
        "    #[0].eval().requires_grad_(False)\n",
        "    perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "    #[0].eval().requires_grad_(True)\n",
        "\n",
        "    cut_size = perceptor.visual.input_resolution\n",
        "    \n",
        "    if self.args.is_gumbel:\n",
        "      e_dim = model.quantize.embedding_dim\n",
        "    else:\n",
        "      e_dim = model.quantize.e_dim\n",
        "        \n",
        "    f = 2**(model.decoder.num_resolutions - 1)\n",
        "   \n",
        "    make_cutouts = flavordict[flavor](cut_size, args.mse_cutn, cut_pow=args.mse_cut_pow,augs=args.augs)\n",
        "\n",
        "    #make_cutouts = MakeCutouts(cut_size, args.mse_cutn, cut_pow=args.mse_cut_pow,augs=args.augs)\n",
        "    if args.altprompts:\n",
        "        self.usealtprompts = True\n",
        "        self.alt_make_cutouts = flavordict[flavor](cut_size, args.mse_cutn, cut_pow=args.alt_mse_cut_pow,augs=args.altaugs)\n",
        "        #self.alt_make_cutouts = MakeCutouts(cut_size, args.mse_cutn, cut_pow=args.alt_mse_cut_pow,augs=args.altaugs)\n",
        "    \n",
        "    if self.args.is_gumbel:\n",
        "      n_toks = model.quantize.n_embed\n",
        "    else:\n",
        "      n_toks = model.quantize.n_e\n",
        "\n",
        "    toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "    sideX, sideY = toksX * f, toksY * f\n",
        "\n",
        "    if self.args.is_gumbel:\n",
        "        z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "        z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "    else:\n",
        "        z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "        z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "    \n",
        "    from PIL import Image\n",
        "    import cv2\n",
        "#-------\n",
        "    working_dir = self.args.folder_name\n",
        "    \n",
        "    if self.args.init_image != \"\":\n",
        "        img_0 = cv2.imread(init_image)\n",
        "        z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "    elif not os.path.isfile(f'{working_dir}/steps/{i:04d}.png'):\n",
        "        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "        if self.args.is_gumbel:\n",
        "            z = one_hot @ model.quantize.embed.weight\n",
        "        else:\n",
        "            z = one_hot @ model.quantize.embedding.weight\n",
        "        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "    else:\n",
        "        if save_all_iterations:\n",
        "            img_0 = cv2.imread(\n",
        "                f'{working_dir}/steps/{i:04d}_{iterations_per_frame}.png')\n",
        "        else:\n",
        "            # Hack to prevent colour inversion on every frame\n",
        "            img_temp = cv2.imread(f'{working_dir}/steps/{i}.png')\n",
        "            imageio.imwrite('inverted_temp.png', img_temp)\n",
        "            img_0 = cv2.imread('inverted_temp.png')\n",
        "        center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "        trans_mat = np.float32(\n",
        "            [[1, 0, translation_x],\n",
        "            [0, 1, translation_y]]\n",
        "        )\n",
        "        rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "\n",
        "        trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "        rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "        transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "\n",
        "        img_0 = cv2.warpPerspective(\n",
        "            img_0,\n",
        "            transformation_matrix,\n",
        "            (img_0.shape[1], img_0.shape[0]),\n",
        "            borderMode=cv2.BORDER_WRAP\n",
        "        )\n",
        "        z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "        \n",
        "        def save_output(i, img, suffix='zoomed'):\n",
        "          filename = \\\n",
        "              f\"{working_dir}/steps/{i:04}{'_' + suffix if suffix else ''}.png\"\n",
        "          imageio.imwrite(filename, np.array(img))\n",
        "\n",
        "        save_output(i, img_0)\n",
        "#-------\n",
        "    if args.init_image:\n",
        "        pil_image = Image.open(args.init_image).convert('RGB')\n",
        "        pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "        z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "    else:\n",
        "        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "        if self.args.is_gumbel:\n",
        "          z = one_hot @ model.quantize.embed.weight\n",
        "        else:\n",
        "          z = one_hot @ model.quantize.embedding.weight\n",
        "        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "    z = EMATensor(z, args.ema_val)\n",
        "    \n",
        "    if args.mse_with_zeros and not args.init_image:\n",
        "        z_orig = torch.zeros_like(z.tensor)\n",
        "    else:\n",
        "        z_orig = z.tensor.clone()\n",
        "    z.requires_grad_(True)\n",
        "    #opt = optim.AdamW(z.parameters(), lr=args.mse_step_size, weight_decay=0.00000000)\n",
        "    if self.normal_flip_optim == True:\n",
        "      if randint(1,2) == 1:\n",
        "        opt = torch.optim.AdamW(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "        print(f\"Using AdamW\")\n",
        "        #opt = Ranger21(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "        #print(f\"Using Ranger21\")\n",
        "      else:\n",
        "        opt = optim.DiffGrad(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "        print(f\"Using DiffGrad\")\n",
        "    else:\n",
        "      opt = torch.optim.AdamW(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "    self.cur_step_size =args.mse_step_size\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                    std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "    pMs = []\n",
        "    altpMs = []\n",
        "\n",
        "    for prompt in args.prompts:\n",
        "        txt, weight, stop = parse_prompt(prompt)\n",
        "        embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "        pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "    \n",
        "    for prompt in args.altprompts:\n",
        "        txt, weight, stop = parse_prompt(prompt)\n",
        "        embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "        altpMs.append(Prompt(embed, weight, stop).to(device))\n",
        "    \n",
        "    from PIL import Image\n",
        "    #Being obsessed with your legacy is kind of stupid to be honest.\n",
        "    for prompt in args.image_prompts:\n",
        "        path, weight, stop = parse_prompt(prompt)\n",
        "        img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = perceptor.encode_image(normalize(batch)).float()\n",
        "        pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "        gen = torch.Generator().manual_seed(seed)\n",
        "        embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "        pMs.append(Prompt(embed, weight).to(device))\n",
        "        if(self.usealtprompts):\n",
        "          altpMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "    self.model, self.perceptor = model, perceptor\n",
        "    self.make_cutouts = make_cutouts\n",
        "    self.imageSize = (sideX, sideY)\n",
        "    self.prompts = pMs\n",
        "    self.altprompts = altpMs\n",
        "    self.opt = opt\n",
        "    self.normalize = normalize\n",
        "    self.z, self.z_orig, self.z_min, self.z_max = z, z_orig, z_min, z_max\n",
        "    self.setup_metadata(seed)\n",
        "    self.mse_weight = self.args.init_weight\n",
        "\n",
        "  def synth(self, z):\n",
        "      if self.args.is_gumbel:\n",
        "          z_q = vector_quantize(z.movedim(1, 3), self.model.quantize.embed.weight).movedim(3, 1)\n",
        "      else:\n",
        "          z_q = vector_quantize(z.movedim(1, 3), self.model.quantize.embedding.weight).movedim(3, 1)\n",
        "      return clamp_with_grad(self.model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "  def add_metadata(self, path, i):\n",
        "    imfile = PngImageFile(path)\n",
        "    meta = PngInfo()\n",
        "    step_meta = {'iterations':i}\n",
        "    step_meta.update(self.metadata)\n",
        "    #meta.add_itxt('vqgan-params', json.dumps(step_meta), zip=True)\n",
        "    imfile.save(path, pnginfo=meta)\n",
        "    #Hey you. This one's for Glooperpogger#7353 on Discord (Gloop has a gun), they are a nice snek\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def checkin(self, i, losses, x):\n",
        "      losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "      if i < args.mse_end:\n",
        "        tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "      else:\n",
        "        tqdm.write(f'i: {i-args.mse_end} ({i}), loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "      tqdm.write(f'cutn: {self.make_cutouts.cutn}, cut_pow: {self.make_cutouts.cut_pow}, step_size: {self.cur_step_size}')\n",
        "      out = self.synth(self.z.average)\n",
        "      if i == self.args.max_iterations:\n",
        "          if save_to_drive== True:\n",
        "              batchpath = self.unique_index(\"./drive/MyDrive/VQGAN_Output/\"+folder_name)\n",
        "          else:\n",
        "              batchpath = self.unique_index(\"./\"+folder_name)\n",
        "          TF.to_pil_image(out[0].cpu()).save(batchpath)\n",
        "      #TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "      #self.add_metadata('progress.png', i)\n",
        "      #display.display(display.Image('progress.png'))\n",
        "      if self.args.png==True:\n",
        "        TF.to_pil_image(out[0].cpu()).save('png_progress.png')\n",
        "        self.add_metadata('png_progress.png', i)\n",
        "        TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "        self.add_metadata('progress.png', i)\n",
        "        #I know it's a mess, BUT, it works, right? RIGHT?!\n",
        "        from PIL import Image, ImageDraw, ImageFilter, ImageEnhance, ImageOps\n",
        "        import PIL.ImageOps    \n",
        "\n",
        "        castle = Image.open(args.init_image).convert('RGB')\n",
        "        #castle = Image.open('castle.png')\n",
        "        castle = ImageEnhance.Brightness(castle)\n",
        "        castle.enhance(100000).save('/content/png_processing/brightness.png')\n",
        "\n",
        "        im = Image.open('/content/png_processing/brightness.png')\n",
        "        im_invert = ImageOps.invert(im)\n",
        "        im_invert.save('/content/png_processing/work.png')\n",
        "\n",
        "        image = Image.open('/content/png_processing/work.png').convert('RGB')\n",
        "        inverted_image = PIL.ImageOps.invert(image)\n",
        "        inverted_image.save('/content/png_processing/last.png')\n",
        "\n",
        "        im_rgb = Image.open('progress.png')\n",
        "        im_a = Image.open('/content/png_processing/last.png').convert('L').resize(im_rgb.size)\n",
        "        im_rgb.putalpha(im_a)\n",
        "\n",
        "        #im_rgb.save('/content/png_progress.png')\n",
        "        im_rgb.save('/content/png_processing/progress.png')\n",
        "        #display(Image.open('/content/png_progress.png').convert('RGB'))\n",
        "        display(Image.open('/content/png_processing/progress.png'))\n",
        "\n",
        "      else:\n",
        "        TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "        self.add_metadata('progress.png', i)\n",
        "        from PIL import Image\n",
        "        display(Image.open('progress.png'))\n",
        "\n",
        "  def unique_index(self, batchpath):\n",
        "      i = 0\n",
        "      while i < 10000:\n",
        "          if os.path.isfile(batchpath+\"/\"+str(i)+\".png\"):\n",
        "              i = i+1\n",
        "          else:\n",
        "              return batchpath+\"/\"+str(i)+\".png\"\n",
        "\n",
        "  def ascend_txt(self, i):\n",
        "      out = self.synth(self.z.tensor)\n",
        "      iii = self.perceptor.encode_image(self.normalize(self.make_cutouts(out))).float()\n",
        "      \n",
        "\n",
        "      result = []\n",
        "      if self.args.init_weight and self.mse_weight > 0:\n",
        "          result.append(F.mse_loss(self.z.tensor, self.z_orig) * self.mse_weight / 2)\n",
        "\n",
        "      for prompt in self.prompts:\n",
        "          result.append(prompt(iii))\n",
        "          \n",
        "      if self.usealtprompts:\n",
        "        iii = self.perceptor.encode_image(self.normalize(self.alt_make_cutouts(out))).float()\n",
        "        for prompt in self.altprompts:\n",
        "          result.append(prompt(iii))\n",
        "      \n",
        "      img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "      img = np.transpose(img, (1, 2, 0))\n",
        "      im_path = f'./steps/{i}.png'\n",
        "      imageio.imwrite(im_path, np.array(img))\n",
        "      self.add_metadata(im_path, i)\n",
        "      return result\n",
        "\n",
        "  def train(self, i,x):\n",
        "      self.opt.zero_grad()\n",
        "      mse_decay = self.args.mse_decay\n",
        "      mse_decay_rate = self.args.mse_decay_rate\n",
        "      lossAll = self.ascend_txt(i)\n",
        "\n",
        "      if i < args.mse_end and i % args.mse_display_freq == 0:\n",
        "        self.checkin(i, lossAll, x)\n",
        "      if i == args.mse_end:\n",
        "        self.checkin(i,lossAll,x)\n",
        "      if i > args.mse_end and (i-args.mse_end) % args.display_freq == 0:\n",
        "        self.checkin(i, lossAll, x)\n",
        "         \n",
        "      loss = sum(lossAll)\n",
        "      loss.backward()\n",
        "      self.opt.step()\n",
        "      with torch.no_grad():\n",
        "          if self.mse_weight > 0 and self.args.init_weight and i > 0 and i%mse_decay_rate == 0:\n",
        "              if self.args.is_gumbel:\n",
        "                self.z_orig = vector_quantize(self.z.average.movedim(1, 3), self.model.quantize.embed.weight).movedim(3, 1)\n",
        "              else:\n",
        "                self.z_orig = vector_quantize(self.z.average.movedim(1, 3), self.model.quantize.embedding.weight).movedim(3, 1)\n",
        "              if self.mse_weight - mse_decay > 0:\n",
        "                  self.mse_weight = self.mse_weight - mse_decay\n",
        "                  print(f\"updated mse weight: {self.mse_weight}\")\n",
        "              else:\n",
        "                  self.mse_weight = 0\n",
        "                  self.make_cutouts = flavordict[flavor](self.perceptor.visual.input_resolution, args.cutn, cut_pow=args.cut_pow, augs = args.augs)\n",
        "                  if self.usealtprompts:\n",
        "                      self.alt_make_cutouts = flavordict[flavor](self.perceptor.visual.input_resolution, args.cutn, cut_pow=args.alt_cut_pow, augs = args.altaugs)\n",
        "                  self.z = EMATensor(self.z.average, args.ema_val)\n",
        "                  self.new_step_size =args.step_size\n",
        "                  self.opt = optim.Adagrad(self.z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "                  print(f\"updated mse weight: {self.mse_weight}\")\n",
        "          if i > args.mse_end:\n",
        "              if args.step_size != args.final_step_size and args.max_iterations > 0:\n",
        "                progress = (i-args.mse_end)/(args.max_iterations)\n",
        "                self.cur_step_size = lerp(step_size, final_step_size,progress)\n",
        "                for g in self.opt.param_groups:\n",
        "                  g['lr'] = self.cur_step_size\n",
        "          #self.z.copy_(self.z.maximum(self.z_min).minimum(self.z_max))\n",
        "\n",
        "  def run(self,x):\n",
        "    i = 0\n",
        "    try:\n",
        "        pbar = tqdm(range(int(args.max_iterations + args.mse_end)))\n",
        "        while True:\n",
        "          self.train(i,x)\n",
        "          if i > 0 and i%args.mse_decay_rate==0 and self.mse_weight > 0:\n",
        "            self.z = EMATensor(self.z.average, args.ema_val)\n",
        "            self.opt = torch.optim.Adagrad(self.z.parameters(), lr=args.mse_step_size, weight_decay=0.00000000)\n",
        "            #self.opt = optim.Adgarad(self.z.parameters(), lr=args.mse_step_size, weight_decay=0.00000000)\n",
        "          if i >= args.max_iterations + args.mse_end:\n",
        "            pbar.close()\n",
        "            break\n",
        "          self.z.update()\n",
        "          i += 1\n",
        "          pbar.update()\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    return i\n",
        "\n",
        "def add_noise(img):\n",
        "\n",
        "\t# Getting the dimensions of the image\n",
        "\trow , col = img.shape\n",
        "\t\n",
        "\t# Randomly pick some pixels in the\n",
        "\t# image for coloring them white\n",
        "\t# Pick a random number between 300 and 10000\n",
        "\tnumber_of_pixels = random.randint(300, 10000)\n",
        "\tfor i in range(number_of_pixels):\n",
        "\t\t\n",
        "\t\t# Pick a random y coordinate\n",
        "\t\ty_coord=random.randint(0, row - 1)\n",
        "\t\t\n",
        "\t\t# Pick a random x coordinate\n",
        "\t\tx_coord=random.randint(0, col - 1)\n",
        "\t\t\n",
        "\t\t# Color that pixel to white\n",
        "\t\timg[y_coord][x_coord] = 255\n",
        "\t\t\n",
        "\t# Randomly pick some pixels in\n",
        "\t# the image for coloring them black\n",
        "\t# Pick a random number between 300 and 10000\n",
        "\tnumber_of_pixels = random.randint(300 , 10000)\n",
        "\tfor i in range(number_of_pixels):\n",
        "\t\t\n",
        "\t\t# Pick a random y coordinate\n",
        "\t\ty_coord=random.randint(0, row - 1)\n",
        "\t\t\n",
        "\t\t# Pick a random x coordinate\n",
        "\t\tx_coord=random.randint(0, col - 1)\n",
        "\t\t\n",
        "\t\t# Color that pixel to black\n",
        "\t\timg[y_coord][x_coord] = 0\n",
        "\t\t\n",
        "\treturn img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhhdWrSxQhwg",
        "cellView": "form"
      },
      "source": [
        "#@title <font color=\"lightgreen\" size=\"+3\">←</font> <font size=\"+2\">💠</font> Selection of models to download <font size=\"+2\">💠</font>\n",
        "#@markdown By default, the notebook downloads the 16384 model from ImageNet. There are others like COCO-Stuff, WikiArt 1024, WikiArt 16384, FacesHQ or S-FLCKR, which are heavy, and if you are not going to use them it would be pointless to download them, so if you want to use them, simply select the models to download.\n",
        "#@markdown It seems for now like S-FLICKR is the best model. Before you use it, know it's really slow.\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "\n",
        "imagenet_1024 = False #@param {type:\"boolean\"}\n",
        "imagenet_16384 = True #@param {type:\"boolean\"}\n",
        "gumbel_8192 = False #@param {type:\"boolean\"}\n",
        "sber_gumbel = False #@param {type:\"boolean\"}\n",
        "imagenet_cin = False #@param {type:\"boolean\"}\n",
        "coco = False #@param {type:\"boolean\"}\n",
        "faceshq = False #@param {type:\"boolean\"}\n",
        "wikiart_1024 = False #@param {type:\"boolean\"}\n",
        "wikiart_16384 = False #@param {type:\"boolean\"}\n",
        "sflckr = False #@param {type:\"boolean\"}\n",
        "##@markdown Experimental models (won't probably work, if you know how to make them work, go ahead :D):\n",
        "#celebahq = False #@param {type:\"boolean\"}\n",
        "#ade20k = False #@param {type:\"boolean\"}\n",
        "#drin = False #@param {type:\"boolean\"}\n",
        "#gumbel = False #@param {type:\"boolean\"}\n",
        "#gumbel_8192 = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if imagenet_1024:\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\n",
        "if imagenet_16384:\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n",
        "if gumbel_8192:\n",
        "  !curl -L -o gumbel_8192.yaml -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #Gumbel 8192\n",
        "  !curl -L -o gumbel_8192.ckpt -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #Gumbel 8192\n",
        "if imagenet_cin:\n",
        "  !curl -L -o imagenet_cin.yaml -C - 'https://app.koofr.net/links/90cbd5aa-ef70-4f5e-99bc-f12e5a89380e?path=%2F2021-04-03T19-39-50_cin_transformer%2Fconfigs%2F2021-04-03T19-39-50-project.yaml' #ImageNet (cIN)\n",
        "  !curl -L -o imagenet_cin.ckpt -C - 'https://app.koofr.net/content/links/90cbd5aa-ef70-4f5e-99bc-f12e5a89380e/files/get/last.ckpt?path=%2F2021-04-03T19-39-50_cin_transformer%2Fcheckpoints%2Flast.ckpt' #ImageNet (cIN)\n",
        "if sber_gumbel:\n",
        "  #!curl -L -o sber_gumbel.yaml -C - 'https://doc-0o-bo-docs.googleusercontent.com/docs/securesc/hj3hebanm6gqj0q3hoarffli0vd5l7r3/p00oqevvd6eh9niid7sff9a3cgjoibni/1635879750000/10065450137691666841/04085140192571190118/1M7RvSoiuKBwpF-98sScKng0lsZnwFebR?e=download&authuser=0' #Gumbel 8192\n",
        "  #!curl -L -o sber_gumbel.ckpt -C - 'https://drive.google.com/u/0/uc?export=download&confirm=kWvZ&id=1WP6Li2Po8xYcQPGMpmaxIlI1yPB5lF5m' #Gumbel 8192\n",
        "  #!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "  #!git clone https://huggingface.co/sberbank-ai/Sber-VQGAN\n",
        "  models_folder = './'\n",
        "  configs_folder = './'\n",
        "\n",
        "  os.makedirs(models_folder, exist_ok=True)\n",
        "  os.makedirs(configs_folder, exist_ok=True)\n",
        "\n",
        "  models_storage = [\n",
        "  {\n",
        "      'id': '1WP6Li2Po8xYcQPGMpmaxIlI1yPB5lF5m',\n",
        "      'name': 'sber_gumbel.ckpt',\n",
        "  },\n",
        "  ]\n",
        "\n",
        "  configs_storage = [{\n",
        "      'id': '1M7RvSoiuKBwpF-98sScKng0lsZnwFebR',\n",
        "      'name': 'sber_gumbel.yaml',\n",
        "  }]\n",
        "  #You're too kind, sir.\n",
        "  url_template = 'https://drive.google.com/uc?id={}'\n",
        "\n",
        "  for item in models_storage:\n",
        "      out_name = os.path.join(models_folder, item['name'])\n",
        "      url = url_template.format(item['id'])\n",
        "      gdown.download(url, out_name, quiet=True)\n",
        "\n",
        "  for item in configs_storage:\n",
        "      out_name = os.path.join(configs_folder, item['name'])\n",
        "      url = url_template.format(item['id'])\n",
        "      gdown.download(url, out_name, quiet=True)\n",
        "if coco:\n",
        "  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n",
        "  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n",
        "if faceshq:\n",
        "  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n",
        "  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n",
        "if wikiart_1024:\n",
        "  #I'm so sorry, I know this is exploiting, but there is no other way.\n",
        "  !curl -L -o wikiart_1024.yaml -C - 'https://github.com/Eleiber/VQGAN-Mirrors/releases/download/0.0.1/wikiart_1024.yaml' #WikiArt 1024\n",
        "  !curl -L -o wikiart_1024.ckpt -C - 'https://github.com/Eleiber/VQGAN-Mirrors/releases/download/0.0.1/wikiart_1024.ckpt' #WikiArt 1024\n",
        "if wikiart_16384: \n",
        "  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml' #WikiArt 16384\n",
        "  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt' #WikiArt 16384\n",
        "if sflckr:\n",
        "  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n",
        "  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR\n",
        "  \n",
        "\n",
        "#None of these work, if you know how to make them work, go ahead. :D - Philipuss\n",
        "#if celebahq:\n",
        "#  !curl -L -o celebahq.yaml -C - 'https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/2021-04-23T18-11-19-project.yaml?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fconfigs%2F2021-04-23T18-11-19-project.yaml&force' #celebahq\n",
        "#  !curl -L -o celebahq.ckpt -C - 'https://app.koofr.net/content/links/6dddf083-40c8-470a-9360-a9dab2a94e96/files/get/last.ckpt?path=%2F2021-04-23T18-11-19_celebahq_transformer%2Fcheckpoints%2Flast.ckpt' #celebahq\n",
        "#if ade20k:\n",
        "#  !curl -L -o ade20k.yaml -C - 'https://app.koofr.net/content/links/0f65c2cd-7102-4550-a2bd-07fd383aac9e/files/get/2020-11-20T21-45-44-project.yaml?path=%2F2020-11-20T21-45-44_ade20k_transformer%2Fconfigs%2F2020-11-20T21-45-44-project.yaml&force' #celebahq\n",
        "#  !curl -L -o ade20k.ckpt -C - 'https://app.koofr.net/content/links/0f65c2cd-7102-4550-a2bd-07fd383aac9e/files/get/last.ckpt?path=%2F2020-11-20T21-45-44_ade20k_transformer%2Fcheckpoints%2Flast.ckpt' #celebahq\n",
        "#if drin:\n",
        "#  !curl -L -o drin.yaml -C - 'https://app.koofr.net/content/links/028f1ba8-404d-42c4-a866-9a8a4eebb40c/files/get/2020-11-20T12-54-32-project.yaml?path=%2F2020-11-20T12-54-32_drin_transformer%2Fconfigs%2F2020-11-20T12-54-32-project.yaml&force' #celebahq\n",
        "#  !curl -L -o drin.ckpt -C - 'https://app.koofr.net/content/links/028f1ba8-404d-42c4-a866-9a8a4eebb40c/files/get/last.ckpt?path=%2F2020-11-20T12-54-32_drin_transformer%2Fcheckpoints%2Flast.ckpt' #celebahq\n",
        "#if gumbel:\n",
        "#  !curl -L -o gumbel.yaml -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #celebahq\n",
        "#  !curl -L -o gumbel.ckpt -C - 'https://heibox.uni-heidelberg.de/d/2e5662443a6b4307b470/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #celebahq\n",
        "\n",
        "#if gumbel_8192:\n",
        "#  !curl -L -o gumbel_8192.yaml -C - 'https://heibox.uni-heidelberg.de/f/b24d14998a8d4f19a34f/?dl=1' #Gumbel\n",
        "#  !curl -L -o gumbel_8192.ckpt -C - 'https://heibox.uni-heidelberg.de/f/34a747d5765840b5a99d/?dl=1' #Gumbel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ut4XJu_Pjo8"
      },
      "source": [
        "\n",
        "### Configure and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdlpRFL8UAlW",
        "cellView": "form"
      },
      "source": [
        "#@title <font color=\"lightgreen\" size=\"+3\">←</font> <font size=\"+2\">🏃‍♂️</font> **Configure & Run** <font size=\"+2\">🏃‍♂️</font>\n",
        "\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "#@markdown >`prompts` is the list of prompts to give to the AI, separated by `|`. With more than one, it will attempt to mix them together. You can add weights to different parts of the prompt by adding a `p:x` at the end of a prompt (before a `|`) where `p` is the prompt and `x` is the weight.\n",
        "prompts = \"A fantasy landscape, by Greg Rutkowski. A lush mountain.:10 | Trending on ArtStation, unreal engine. 4K HD, realism.:6.3\" #@param {type:\"string\"}\n",
        "width =  480#@param {type:\"number\"}\n",
        "height = 336 #@param {type:\"number\"}\n",
        "model = \"ImageNet 16384\" #@param ['ImageNet 16384', 'ImageNet 1024', \"Gumbel 8192\", \"Sber Gumbel\", \"imagenet_cin\", 'WikiArt 1024', 'WikiArt 16384', 'COCO-Stuff', 'FacesHQ', 'S-FLCKR']\n",
        "\n",
        "if model == \"Gumbel 8192\":\n",
        "  is_gumbel = True\n",
        "elif model == \"Sber Gumbel\":\n",
        "  is_gumbel = True\n",
        "else:\n",
        "  is_gumbel = False\n",
        "\n",
        "##@markdown The flavor effects the output greatly. Each has it's own characteristics and depending on what you choose, you'll get a widely different result with the same prompt and seed. Ginger is the default, nothing special. Cumin results more of a painting, while Holywater makes everythng super funky and/or colorful. Custom is a custom flavor, use the utilities above.\n",
        "flavor = 'holywater' #@param [\"ginger\", \"cumin\", \"holywater\", \"zynth\", \"wyvern\", \"custom\"]\n",
        "template = 'Balanced' #@param [\"none\", \"Balanced\", \"Better - Fast\", \"Better - Slow\", \"Detailed\", \"Consistent Creativity\", \"Realistic\", \"Smooth\", \"Subtle MSE\", \"Hyper Fast Results\", \"Pixelart\", \"Movie Poster\", \"flag\", \"planet\", \"creature\", \"human\", \"Size: Square\", \"Size: Landscape\", \"Size: Poster\", \"Negative Prompt\"]\n",
        "##@markdown To use initial or target images, upload it on the left in the file browser. You can also use previous outputs by putting its path below, e.g. `batch_01/0.png`. If your previous output is saved to drive, you can use the checkbox so you don't have to type the whole path.\n",
        "init = 'default noise' #@param [\"default noise\", \"image\", \"random image\", \"salt and pepper noise\", \"salt and pepper noise on init image\"]\n",
        "init_image = \"\"#@param {type:\"string\"}\n",
        "\n",
        "if init == \"random image\":\n",
        "  url = \"https://picsum.photos/\" + str(width) + \"/\" + str(height) + \"?blur=\" + str(random.randrange(5, 10))\n",
        "  urllib.request.urlretrieve(url, \"Init_Img/Image.png\")\n",
        "  init_image = \"Init_Img/Image.png\"\n",
        "elif init == \"random image clear\":\n",
        "  url = \"https://source.unsplash.com/random/\" + str(width) + \"x\" + str(height)\n",
        "  urllib.request.urlretrieve(url, \"Init_Img/Image.png\")\n",
        "  init_image = \"Init_Img/Image.png\"\n",
        "elif init == \"random image clear 2\":\n",
        "  url = \"https://loremflickr.com/\" + str(width) + \"/\" + str(height)\n",
        "  urllib.request.urlretrieve(url, \"Init_Img/Image.png\")\n",
        "  init_image = \"Init_Img/Image.png\"\n",
        "elif init == \"salt and pepper noise\":\n",
        "  urllib.request.urlretrieve(\"https://i.stack.imgur.com/olrL8.png\", \"Init_Img/Image.png\")\n",
        "  import cv2\n",
        "  img = cv2.imread('Init_Img/Image.png', 0)\n",
        "  cv2.imwrite('Init_Img/Image.png', add_noise(img))\n",
        "  init_image = \"Init_Img/Image.png\"\n",
        "elif init == \"salt and pepper noise on init image\":\n",
        "  img = cv2.imread(init_image, 0)\n",
        "  cv2.imwrite('Init_Img/Image.png', add_noise(img))\n",
        "  init_image = \"Init_Img/Image.png\"\n",
        "elif init == \"perlin noise\":\n",
        "  #For some reason Colab started crashing from this\n",
        "  import noise\n",
        "  import numpy as np\n",
        "  from PIL import Image\n",
        "  shape = (width, height)\n",
        "  scale = 100\n",
        "  octaves = 6\n",
        "  persistence = 0.5\n",
        "  lacunarity = 2.0\n",
        "  seed = np.random.randint(0,100000)\n",
        "  world = np.zeros(shape)\n",
        "  for i in range(shape[0]):\n",
        "      for j in range(shape[1]):\n",
        "          world[i][j] = noise.pnoise2(i/scale, j/scale, octaves=octaves, persistence=persistence, lacunarity=lacunarity, repeatx=1024, repeaty=1024, base=seed)\n",
        "  Image.fromarray(prep_world(world)).convert(\"L\").save(\"Init_Img/Image.png\")\n",
        "  init_image = \"Init_Img/Image.png\"\n",
        "elif init == \"black and white\":\n",
        "  url = \"https://www.random.org/bitmaps/?format=png&width=300&height=300&zoom=1\"\n",
        "  urllib.request.urlretrieve(url, \"Init_Img/Image.png\")\n",
        "  init_image = \"Init_Img/Image.png\"\n",
        "\n",
        "\n",
        "\n",
        "seed = -1#@param {type:\"number\"}\n",
        "transparent_png = False #@param {type:\"boolean\"}\n",
        "##@markdown Only the **prompts**, **width**, **height** and **model** work on pixel art.\n",
        "use_CLIPIT = False #@param {type:\"boolean\"}\n",
        "Pixel_Art = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <font size=\"+3\">⚠</font> **ADVANCED SETTINGS** <font size=\"+3\">⚠</font>\n",
        "#@markdown ---\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown >`folder_name` is the name of the folder you want to output your result(s) to. Previous outputs will NOT be overwritten. By default, it will be saved to the colab's root folder, but the `save_to_drive` checkbox will save it to `MyDrive\\VQGAN_Output` instead.\n",
        "folder_name = \"Output\"#@param {type:\"string\"}\n",
        "save_to_drive = False #@param {type:\"boolean\"}\n",
        "prompt_experiment = \"None\" #@param ['None', 'Fever Dream', 'Philipuss’s Basement', 'Vivid Turmoil', 'Mad Dad', 'Platinum', 'Negative Energy']\n",
        "if prompt_experiment == \"Fever Dream\":\n",
        "  prompts = \"<|startoftext|>\" + prompts + \"<|endoftext|>\"\n",
        "elif prompt_experiment == \"Vivid Turmoil\":\n",
        "  prompts = prompts.replace(\" \", \"¡\")\n",
        "  prompts = \"¬\" + prompts + \"®\"\n",
        "elif prompt_experiment == \"Mad Dad\":\n",
        "  prompts = prompts.replace(\" \", '\\\\s+')\n",
        "elif prompt_experiment == \"Platinum\":\n",
        "  prompts = \"~!\" + prompts + \"!~\"\n",
        "  prompts = prompts.replace(\" \", '</w>')\n",
        "elif prompt_experiment == \"Philipuss’s Basement\":\n",
        "  prompts = \"<|startoftext|>\" + prompts\n",
        "  prompts = prompts.replace(\" \", \"<|endoftext|><|startoftext|>\")\n",
        "  \n",
        "clip_model = \"ViT-B/32\" #@param [\"ViT-B/32\", \"ViT-B/16\", \"RN50x16\", \"RN50x4\", \"RN101\", \"RN50\"]\n",
        "\n",
        "#@markdown >Target images work like prompts, write the name of the image. You can add multiple target images by seperating them with a `|`.\n",
        "target_images = \"\"#@param {type:\"string\"}\n",
        "\n",
        "#@markdown ><font size=\"+2\">☢</font> Advanced values. Values of cut_pow below 1 prioritize structure over detail, and vice versa for above 1. Step_size affects how wild the change between iterations is, and if final_step_size is not 0, step_size will interpolate towards it over time.\n",
        "#@markdown >Cutn affects on 'Creativity': less cutout will lead to more random/creative results, sometimes barely readable, while higher values (90+) lead to very stable, photo-like outputs\n",
        "cutn =  130#@param {type:\"number\"}\n",
        "cut_pow = 1#@param {type:\"number\"}\n",
        "#@markdown >Step_size is like weirdness. Lower: more accurate/realistic, slower; Higher: less accurate/more funky, faster.\n",
        "step_size = 0.12#@param {type:\"number\"}\n",
        "final_step_size = 0#@param {type:\"number\"} \n",
        "if final_step_size <= 0: final_step_size = step_size\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown >EMA maintains a moving average of trained parameters. The number below is the rate of decay (higher means slower).\n",
        "ema_val =  0.98#@param {type:\"number\"}\n",
        "\n",
        "init_image_in_drive = False #@param {type:\"boolean\"}\n",
        "if init_image_in_drive and init_image:\n",
        "    init_image = '/content/drive/MyDrive/VQGAN_Output/' + init_image\n",
        "\n",
        "images_interval =  10#@param {type:\"number\"}\n",
        "\n",
        "#I think you should give \"Free Thoughts on the Proceedings of the Continental Congress\" a read, really funny and actually well-written, Hamilton presented it in a bad light IMO.\n",
        "\n",
        "#@markdown >iterations excludes iterations spent during the mse phase, if it is being used. The total iterations will be more if `mse_decay_rate` is more than 0.\n",
        "iterations = 100#@param {type:\"number\"}\n",
        "batch_size =  1#@param {type:\"number\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <font size=\"+1\">🔮</font> MSE Regulization. <font size=\"+1\">🔮</font>\n",
        "#Based off of this notebook: https://colab.research.google.com/drive/1gFn9u3oPOgsNzJWEFmdK-N9h_y65b8fj?usp=sharing - already in credits\n",
        "use_mse = True #@param {type:\"boolean\"}\n",
        "mse_images_interval = images_interval\n",
        "mse_init_weight =  0.2#@param {type:\"number\"}\n",
        "mse_decay_rate =  110#@param {type:\"number\"}\n",
        "mse_epoches = iterations\n",
        "##@param {type:\"number\"}\n",
        "\n",
        "#@markdown >Overwrites the usual values during the mse phase if included. If any value is 0, its normal counterpart is used instead.\n",
        "mse_with_zeros = True #@param {type:\"boolean\"}\n",
        "mse_step_size = 0.87 #@param {type:\"number\"}\n",
        "mse_cutn =  42#@param {type:\"number\"}\n",
        "mse_cut_pow = 0.75 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown >normal_flip_optim flips between two optimizers during the normal (not MSE) phase. It can improve quality, but it's kind of experimental, use at your own risk.\n",
        "normal_flip_optim = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown >`altprompts` is a set of prompts that take in a different augmentation pipeline, and can have their own cut_pow. At the moment, the default \"alt augment\" settings flip the picture cutouts upside down before evaluating. This can be good for optical illusion images. If either cut_pow value is 0, it will use the same value as the normal prompts.\n",
        "altprompts = \"\" #@param {type:\"string\"}\n",
        "alt_cut_pow = 0 #@param {type:\"number\"}\n",
        "alt_mse_cut_pow = 0 #@param {type:\"number\"}\n",
        "#altprompt_type = \"upside-down\" #@param ['upside-down', 'as']\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown # <font size=\"+3\">➰</font> **CLIPIT** <font size=\"+3\">➰</font>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "use_zoom = False #@param {type:\"boolean\"}\n",
        "key_frames = False #@param {type:\"boolean\"}\n",
        "text_prompts = \"10:(Apple: 1| Orange: 0), 20: (Apple: 0| Orange: 1| Peach: 1)\" #@param {type:\"string\"}\n",
        "interval =  1#@param {type:\"number\"}\n",
        "max_frames = 50#@param {type:\"number\"}\n",
        "angle = \"10: (0), 30: (10), 50: (0)\"#@param {type:\"string\"}\n",
        "zoom = \"10: (1), 30: (1.2), 50: (1)\"#@param {type:\"string\"}\n",
        "translation_x = \"0: (0)\"#@param {type:\"string\"}\n",
        "translation_y = \"0: (0)\"#@param {type:\"string\"}\n",
        "iterations_per_frame = \"0: (10)\"#@param {type:\"string\"}\n",
        "save_all_iterations = False#@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown On an unrelated note, if you get any errors while running this, restart the runtime and run the first cell again. If that doesn't work either, message me on Discord (Philipuss#4066).\n",
        "\n",
        "model_names={'ImageNet 16384': 'vqgan_imagenet_f16_16384', 'ImageNet 1024': 'vqgan_imagenet_f16_1024', \"Gumbel 8192\": \"gumbel_8192\", \"Sber Gumbel\": \"sber_gumbel\", 'imagenet_cin': 'imagenet_cin', 'WikiArt 1024': 'wikiart_1024', 'WikiArt 16384': 'wikiart_16384', 'COCO-Stuff': 'coco', 'FacesHQ': 'faceshq', 'S-FLCKR': 'sflckr'}\n",
        "\n",
        "if template == \"Better - Fast\":\n",
        "  prompts = prompts + \". Detailed artwork. ArtStationHQ. unreal engine. 4K HD.\"\n",
        "elif template == \"Better - Slow\":\n",
        "  prompts = prompts + \". Detailed artwork. Trending on ArtStation. unreal engine. | Rendered in Maya. \" + prompts + \". 4K HD.\"\n",
        "elif template == \"Movie Poster\":\n",
        "  prompts = prompts + \". Movie poster. Rendered in unreal engine. ArtStationHQ.\"\n",
        "  width = 400\n",
        "  height = 592\n",
        "elif template == 'flag':\n",
        "  prompts = \"A photo of a flag of the country \" + prompts + \" | Flag of \" + prompts + \". White background.\"\n",
        "  #import cv2\n",
        "  #img = cv2.imread('templates/flag.png', 0)\n",
        "  #cv2.imwrite('templates/final_flag.png', add_noise(img))\n",
        "  init_image = \"templates/flag.png\"\n",
        "  transparent_png = True\n",
        "elif template == 'planet':\n",
        "  import cv2\n",
        "  img = cv2.imread('templates/planet.png', 0)\n",
        "  cv2.imwrite('templates/final_planet.png', add_noise(img))\n",
        "  prompts = \"A photo of the planet \" + prompts + \". Planet in the middle with black background. | The planet of \" + prompts + \". Photo of a planet. Black background. Trending on ArtStation. | Colorful.\"\n",
        "  init_image = \"templates/final_planet.png\"\n",
        "elif template == 'creature':\n",
        "  #import cv2\n",
        "  #img = cv2.imread('templates/planet.png', 0)\n",
        "  #cv2.imwrite('templates/final_planet.png', add_noise(img))\n",
        "  prompts = \"A photo of a creature with \" + prompts + \". Animal in the middle with white background. | The creature has \" + prompts + \". Photo of a creature/animal. White background. Detailed image of a creature. | White background.\"\n",
        "  init_image = \"templates/creature.png\"\n",
        "  #transparent_png = True\n",
        "elif template == 'Detailed':\n",
        "  prompts = prompts + \", by Puer Udger. Detailed artwork, trending on artstation. 4K HD, realism.\"\n",
        "  flavor = \"cumin\"\n",
        "elif template == \"human\":\n",
        "  init_image = \"/content/templates/human.png\"\n",
        "elif template == \"Realistic\":\n",
        "  cutn = 200\n",
        "  step_size = 0.03\n",
        "  cut_pow = 0.2\n",
        "  flavor = \"holywater\"\n",
        "elif template == \"Consistent Creativity\":\n",
        "  flavor = \"cumin\"\n",
        "  cut_pow = 0.01\n",
        "  cutn = 136\n",
        "  step_size = 0.08\n",
        "  mse_step_size = 0.41\n",
        "  mse_cut_pow = 0.3\n",
        "  ema_val = 0.99\n",
        "  normal_flip_optim = False\n",
        "elif template == \"Pixelart\":\n",
        "  Pixel_Art = True\n",
        "  use_CLIPIT = True\n",
        "  prompts = prompts + \" #pixelart\"\n",
        "elif template == \"Smooth\":\n",
        "  flavor = \"wyvern\"\n",
        "  step_size = 0.10\n",
        "  cutn = 120\n",
        "  normal_flip_optim = False\n",
        "elif template == \"Subtle MSE\":\n",
        "  mse_init_weight = 0.07\n",
        "  mse_decay_rate = 130\n",
        "  mse_step_size = 0.2\n",
        "  mse_cutn = 100\n",
        "  mse_cut_pow = 0.6\n",
        "elif template == \"Balanced\":\n",
        "  cutn = 130\n",
        "  cut_pow = 1\n",
        "  step_size = 0.16\n",
        "  final_step_size = 0\n",
        "  ema_val = 0.97\n",
        "  use_mse = True\n",
        "  mse_init_weight = 0.2\n",
        "  mse_decay_rate = 130\n",
        "  mse_with_zeros = True\n",
        "  mse_step_size = 0.9\n",
        "  mse_cutn = 50\n",
        "  mse_cut_pow = 0.8\n",
        "  normal_flip_optim = True\n",
        "elif template == \"Size: Square\":\n",
        "  width = 450\n",
        "  height = 450\n",
        "elif template == \"Size: Landscape\":\n",
        "  width = 480\n",
        "  height = 336\n",
        "elif template == \"Size: Poster\":\n",
        "  width = 336\n",
        "  height = 480\n",
        "elif template == \"Negative Prompt\":\n",
        "  prompts = prompts.replace(\":\", \":-\")\n",
        "  prompts = prompts.replace(\":--\", \":\")\n",
        "elif template == \"Hyper Fast Results\":\n",
        "  step_size = 1\n",
        "  ema_val = 0.3\n",
        "  cutn = 30\n",
        "\n",
        "#How does Hamilton, an arrogant, immigrant, orphan, bastard, whore's son, somehow endorse Thomas Jefferson?\n",
        "#Like literally, why? Neither his, nor Burr's, letters tell exactly why our stupid Alex endorsed Jefferson over Burr. \n",
        "#Burr was objectively better AND was a firnd of Hamilton, it's odd.\n",
        "\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))\n",
        "\n",
        "\n",
        "\n",
        "if use_CLIPIT==True:\n",
        "  def parse_key_frames(string, prompt_parser=None):\n",
        "      import re\n",
        "      pattern = r'((?P<frame>[0-9]+):[\\s]*[\\(](?P<param>[\\S\\s]*?)[\\)])'\n",
        "      frames = dict()\n",
        "      for match_object in re.finditer(pattern, string):\n",
        "          frame = int(match_object.groupdict()['frame'])\n",
        "          param = match_object.groupdict()['param']\n",
        "          if prompt_parser:\n",
        "              frames[frame] = prompt_parser(param)\n",
        "          else:\n",
        "              frames[frame] = param\n",
        "      return frames\n",
        "\n",
        "  def get_inbetweens(key_frames, integer=False):\n",
        "      key_frame_series = pd.Series([np.nan for a in range(max_frames)])\n",
        "      for i, value in key_frames.items():\n",
        "          key_frame_series[i] = value\n",
        "      key_frame_series = key_frame_series.astype(float)\n",
        "      key_frame_series = key_frame_series.interpolate(limit_direction='both')\n",
        "      if integer:\n",
        "          return key_frame_series.astype(int)\n",
        "      return key_frame_series\n",
        "\n",
        "  def split_key_frame_text_prompts(frames):\n",
        "      prompt_dict = dict()\n",
        "      for i, parameters in frames.items():\n",
        "          prompts = parameters.split('|')\n",
        "          for prompt in prompts:\n",
        "              string, value = prompt.split(':')\n",
        "              string = string.strip()\n",
        "              value = float(value.strip())\n",
        "              if string in prompt_dict:\n",
        "                  prompt_dict[string][i] = value\n",
        "              else:\n",
        "                  prompt_dict[string] = {i: value}\n",
        "      prompt_series_dict = dict()\n",
        "      for prompt, values in prompt_dict.items():\n",
        "          value_string = (\n",
        "              ', '.join([f'{value}: ({values[value]})' for value in values])\n",
        "          )\n",
        "          prompt_series = get_inbetweens(parse_key_frames(value_string))\n",
        "          prompt_series_dict[prompt] = prompt_series\n",
        "      prompt_list = []\n",
        "      for i in range(max_frames):\n",
        "          prompt_list.append(\n",
        "              ' | '.join(\n",
        "                  [f'{prompt}: {prompt_series_dict[prompt][i]}'\n",
        "                  for prompt in prompt_series_dict]\n",
        "              )\n",
        "          )\n",
        "      return prompt_list\n",
        "\n",
        "  if key_frames:\n",
        "      text_prompts_series = split_key_frame_text_prompts(\n",
        "          parse_key_frames(text_prompts)\n",
        "      )\n",
        "      target_images_series = split_key_frame_text_prompts(\n",
        "          parse_key_frames(target_images)\n",
        "      )\n",
        "      angle_series = get_inbetweens(parse_key_frames(angle))\n",
        "      zoom_series = get_inbetweens(parse_key_frames(zoom))\n",
        "      translation_x_series = get_inbetweens(parse_key_frames(translation_x))\n",
        "      translation_y_series = get_inbetweens(parse_key_frames(translation_y))\n",
        "      iterations_per_frame_series = get_inbetweens(\n",
        "          parse_key_frames(iterations_per_frame), integer=True\n",
        "      )\n",
        "  else:\n",
        "      text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "      if text_prompts == ['']:\n",
        "          text_prompts = []\n",
        "      if target_images == \"None\" or not target_images:\n",
        "          target_images = []\n",
        "      else:\n",
        "          target_images = target_images.split(\"|\")\n",
        "          target_images = [image.strip() for image in target_images]\n",
        "\n",
        "      angle = float(angle)\n",
        "      zoom = float(zoom)\n",
        "      translation_x = float(translation_x)\n",
        "      translation_y = float(translation_y)\n",
        "      iterations_per_frame = int(iterations_per_frame)\n",
        "\n",
        "      !nvidia-smi -caa\n",
        "      for var in ['device', 'model', 'perceptor', 'z']:\n",
        "        try:\n",
        "            del globals()[var]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "      try:\n",
        "          import gc\n",
        "          gc.collect()\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "      try:\n",
        "          torch.cuda.empty_cache()\n",
        "      except:\n",
        "          pass\n",
        "\n",
        "      device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      print('Using device:', device)\n",
        "      if not key_frames:\n",
        "          if text_prompts:\n",
        "              print('Using text prompts:', text_prompts)\n",
        "          if target_images:\n",
        "              print('Using image prompts:', target_images)\n",
        "      if args.seed is None:\n",
        "          seed = torch.seed()\n",
        "      else:\n",
        "          seed = args.seed\n",
        "      torch.manual_seed(seed)\n",
        "      print('Using seed:', seed)\n",
        "\n",
        "      model = load_vqgan_model(f'{args.vqgan_model}.yaml', f'{args.vqgan_model}.ckpt').to(device)\n",
        "      perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "      cut_size = perceptor.visual.input_resolution\n",
        "      e_dim = model.quantize.e_dim\n",
        "      f = 2**(model.decoder.num_resolutions - 1)\n",
        "      make_cutouts = flavordict[flavor](cut_size, args.cutn, cut_pow=args.cut_pow, augs=args.augs)\n",
        "      #make_cutouts = flavordict[flavor](cut_size, augs=args.augs, args.cutn, cut_pow=args.cut_pow)\n",
        "      n_toks = model.quantize.n_e\n",
        "      toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "      sideX, sideY = toksX * f, toksY * f\n",
        "      z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "      z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "      stop_on_next_loop = False  # Make sure GPU memory doesn't get corrupted from cancelling the run mid-way through, allow a full frame to complete\n",
        "\n",
        "      for i in range(max_frames):\n",
        "          if stop_on_next_loop:\n",
        "            break\n",
        "          if key_frames:\n",
        "              text_prompts = text_prompts_series[i]\n",
        "              text_prompts = [phrase.strip() for phrase in text_prompts.split(\"|\")]\n",
        "              if text_prompts == ['']:\n",
        "                  text_prompts = []\n",
        "              args.prompts = text_prompts\n",
        "\n",
        "              target_images = target_images_series[i]\n",
        "\n",
        "              if target_images == \"None\" or not target_images:\n",
        "                  target_images = []\n",
        "              else:\n",
        "                  target_images = target_images.split(\"|\")\n",
        "                  target_images = [image.strip() for image in target_images]\n",
        "              args.image_prompts = target_images\n",
        "\n",
        "              angle = angle_series[i]\n",
        "              zoom = zoom_series[i]\n",
        "              translation_x = translation_x_series[i]\n",
        "              translation_y = translation_y_series[i]\n",
        "              iterations_per_frame = iterations_per_frame_series[i]\n",
        "              print(\n",
        "                  f'text_prompts: {text_prompts}'\n",
        "                  f'angle: {angle}',\n",
        "                  f'zoom: {zoom}',\n",
        "                  f'translation_x: {translation_x}',\n",
        "                  f'translation_y: {translation_y}',\n",
        "                  f'iterations_per_frame: {iterations_per_frame}'\n",
        "              )\n",
        "          try:\n",
        "              if i == 0 and init_image != \"\":\n",
        "                  img_0 = cv2.imread(init_image)\n",
        "                  z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "              elif i == 0 and not os.path.isfile(f'{folder_name}/{i:04d}.png'):\n",
        "                  one_hot = F.one_hot(\n",
        "                      torch.randint(n_toks, [toksY * toksX], device=device), n_toks\n",
        "                  ).float()\n",
        "                  z = one_hot @ model.quantize.embedding.weight\n",
        "                  z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "              else:\n",
        "                  if save_all_iterations:\n",
        "                      img_0 = cv2.imread(\n",
        "                          f'{folder_name}/steps/{i:04d}_{iterations_per_frame}.png')\n",
        "                  else:\n",
        "                      # Hack to prevent colour inversion on every frame\n",
        "                      img_temp = cv2.imread(f'{folder_name}/{i:04d}.png')\n",
        "                      imageio.imwrite('inverted_temp.png', img_temp)\n",
        "                      img_0 = cv2.imread('inverted_temp.png')\n",
        "                  center = (1*img_0.shape[1]//2, 1*img_0.shape[0]//2)\n",
        "                  trans_mat = np.float32(\n",
        "                      [[1, 0, translation_x],\n",
        "                      [0, 1, translation_y]]\n",
        "                  )\n",
        "                  rot_mat = cv2.getRotationMatrix2D( center, angle, zoom )\n",
        "\n",
        "                  trans_mat = np.vstack([trans_mat, [0,0,1]])\n",
        "                  rot_mat = np.vstack([rot_mat, [0,0,1]])\n",
        "                  transformation_matrix = np.matmul(rot_mat, trans_mat)\n",
        "\n",
        "                  img_0 = cv2.warpPerspective(\n",
        "                      img_0,\n",
        "                      transformation_matrix,\n",
        "                      (img_0.shape[1], img_0.shape[0]),\n",
        "                      borderMode=cv2.BORDER_WRAP\n",
        "                  )\n",
        "                  z, *_ = model.encode(TF.to_tensor(img_0).to(device).unsqueeze(0) * 2 - 1)\n",
        "              i += 1\n",
        "\n",
        "              z_orig = z.clone()\n",
        "              z.requires_grad_(True)\n",
        "              opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "              normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                              std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "              pMs = []\n",
        "\n",
        "              for prompt in args.prompts:\n",
        "                  txt, weight, stop = parse_prompt(prompt)\n",
        "                  embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "                  pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "              for prompt in args.image_prompts:\n",
        "                  path, weight, stop = parse_prompt(prompt)\n",
        "                  img = resize_image(Image.open(path).convert('RGB'), (sideX, sideY))\n",
        "                  batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "                  embed = perceptor.encode_image(normalize(batch)).float()\n",
        "                  pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "              for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "                  gen = torch.Generator().manual_seed(seed)\n",
        "                  embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "                  pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "              def synth(z):\n",
        "                  z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "                  return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "              def add_xmp_data(filename):\n",
        "                  imagen = ImgTag(filename=filename)\n",
        "                  imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "                  if args.prompts:\n",
        "                      imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', \" | \".join(args.prompts), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "                  else:\n",
        "                      imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', 'None', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "                  imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'i', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "                  imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_name, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "                  imagen.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed',str(seed) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "                  imagen.close()\n",
        "\n",
        "              def add_stegano_data(filename):\n",
        "                  data = {\n",
        "                      \"title\": \" | \".join(args.prompts) if args.prompts else None,\n",
        "                      \"notebook\": \"VQGAN+CLIP\",\n",
        "                      \"i\": i,\n",
        "                      \"model\": model_name,\n",
        "                      \"seed\": str(seed),\n",
        "                  }\n",
        "                  lsb.hide(filename, json.dumps(data)).save(filename)\n",
        "\n",
        "              @torch.no_grad()\n",
        "              def checkin(i, losses):\n",
        "                  losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "                  tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "                  out = synth(z)\n",
        "                  TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "                  add_stegano_data('progress.png')\n",
        "                  add_xmp_data('progress.png')\n",
        "                  display.display(display.Image('progress.png'))\n",
        "\n",
        "              def save_output(i, img, suffix=None):\n",
        "                  filename = \\\n",
        "                      f\"{folder_name}/{i:04}{'_' + suffix if suffix else ''}.png\"\n",
        "                  imageio.imwrite(filename, np.array(img))\n",
        "                  add_stegano_data(filename)\n",
        "                  add_xmp_data(filename)\n",
        "\n",
        "              def ascend_txt(i, save=True, suffix=None):\n",
        "                  out = synth(z)\n",
        "                  iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "                  result = []\n",
        "\n",
        "                  if args.init_weight:\n",
        "                      result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "                  for prompt in pMs:\n",
        "                      result.append(prompt(iii))\n",
        "                  img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "                  img = np.transpose(img, (1, 2, 0))\n",
        "                  if save:\n",
        "                      save_output(i, img, suffix=suffix)\n",
        "                  return result\n",
        "\n",
        "              def train(i, save=True, suffix=None):\n",
        "                  opt.zero_grad()\n",
        "                  lossAll = ascend_txt(i, save=save, suffix=suffix)\n",
        "                  if i % args.display_freq == 0 and save:\n",
        "                      checkin(i, lossAll)\n",
        "                  loss = sum(lossAll)\n",
        "                  loss.backward()\n",
        "                  opt.step()\n",
        "                  with torch.no_grad():\n",
        "                      z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "              with tqdm() as pbar:\n",
        "                  if iterations_per_frame == 0:\n",
        "                      save_output(i, img_0)\n",
        "                  j = 1\n",
        "                  while True:\n",
        "                      suffix = (str(j) if save_all_iterations else None)\n",
        "                      if j >= iterations_per_frame:\n",
        "                          train(i, save=True, suffix=suffix)\n",
        "                          break\n",
        "                      if save_all_iterations:\n",
        "                          train(i, save=True, suffix=suffix)\n",
        "                      else:\n",
        "                          train(i, save=False, suffix=suffix)\n",
        "                      j += 1\n",
        "                      pbar.update()\n",
        "          except KeyboardInterrupt:\n",
        "            stop_on_next_loop = True\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if Pixel_Art or use_CLIPIT:\n",
        "  # Simple setup\n",
        "  import CLIPIT\n",
        "  from IPython import display\n",
        "  import shutil, os\n",
        "  import random\n",
        "  from tqdm.notebook import tqdm\n",
        "\n",
        "  def unique_index(batchpath):\n",
        "    i = 0\n",
        "    while i < 100000:\n",
        "      if os.path.isfile(batchpath+str(i)+\".png\"):\n",
        "        i = i+1\n",
        "      else:\n",
        "        return batchpath+str(i)+\".png\"\n",
        "\n",
        "  if save_to_drive:\n",
        "    path = '/content/drive/MyDrive/ClipIt/'\n",
        "  else:\n",
        "    path = '/content/Output/'\n",
        "  if os.path.isdir(path) == False:\n",
        "    os.makedirs(path)\n",
        "\n",
        "  file_name = unique_index(path)\n",
        "  if init_image == \"\":\n",
        "    init_image = None\n",
        "  tqdm.write(file_name)\n",
        "\n",
        "  prompt_list = [prompts]\n",
        "\n",
        "  from CLIPIT import generate\n",
        "\n",
        "\n",
        "  try:\n",
        "    for prompt in prompt_list:\n",
        "      file_name = unique_index(path)\n",
        "\n",
        "      generate.reset_settings()\n",
        "\n",
        "      if Pixel_Art:\n",
        "        clipit.add_settings(use_pixeldraw=True)\n",
        "        prompts = prompts + \" #pixelart\"\n",
        "\n",
        "      generate.add_settings(prompts=prompts, vqgan_model=\"imagenet_f16_16384\")\n",
        "      generate.add_settings(init_image = None)\n",
        "      generate.add_settings(quality=\"better\", aspect = \"square\")\n",
        "\n",
        "      #TODO: set this value automagically\n",
        "      clip_model_count = 3\n",
        "\n",
        "      #Note: raising scale can cause OOM errors unless you lower num_cuts down below. I advise you leave this alone.\n",
        "      generate.add_settings(scale = 2.8, aspect=\"square\")\n",
        "      generate.add_settings(display_freq = images_interval)\n",
        "      generate.add_settings(cudnn_determinism=True)\n",
        "      generate.add_settings(prompts=prompt)\n",
        "\n",
        "      # Tweaking these values can affect \"early\" generation. Each MSE epoch has the AI try to do a quick \"draft\" of the image, which can help with final image structure. Decay is the length of each epoch.\n",
        "      mse_epochs = mse_epoches\n",
        "      mse_decay = mse_decay_rate\n",
        "      \n",
        "      mse_weight = 1.25\n",
        "      mse_base_weight = mse_init_weight\n",
        "      mse_epoch = mse_epoches\n",
        "\n",
        "      base_iter = iterations\n",
        "\n",
        "      max_iter = base_iter + mse_decay * mse_epochs\n",
        "      mse_iter = mse_epochs * mse_decay\n",
        "      \n",
        "      generate.add_settings(init_weight = (mse_weight + mse_base_weight) * clip_model_count)\n",
        "      generate.add_settings(optimiser='Adagrad', step_size= step_size, num_cuts=cutn, iterations = iterations)\n",
        "      generate.add_settings(init_image = init_image)\n",
        "      settings = generate.apply_settings()\n",
        "      generate.do_init(settings)\n",
        "      \n",
        "      i=0\n",
        "      pbar = tqdm(range(max_iter+1))\n",
        "      for i in range(max_iter+1):\n",
        "        generate.i = i\n",
        "        generate.train(settings, i)\n",
        "        \n",
        "        if i > 0 and i <= mse_iter and i % mse_decay == 0:\n",
        "          mse_epoch = mse_epoch+1\n",
        "          settings.init_weight = ((mse_epochs-mse_epoch)/mse_epochs * mse_weight + mse_base_weight) *  clip_model_count\n",
        "          tqdm.write(f'New weight: {settings.init_weight}')\n",
        "          if mse_epoch == mse_epochs:\n",
        "            tqdm.write(f'DiffGrad loaded')\n",
        "            settings.init_weight = 0\n",
        "            generate.opt = generate.DiffGrad([generate.z],lr = 1)\n",
        "          else:\n",
        "            generate.opt = generate.optim.Adagrad([generate.z],lr = 2)\n",
        "        elif i > mse_iter:\n",
        "          progress = ((i-mse_iter) / base_iter) ** 1.2\n",
        "          for g in generate.opt.param_groups:\n",
        "            g['lr'] = 1 - 0.9 * progress\n",
        "        pbar.update()\n",
        "\n",
        "      shutil.copy('/content/output.png', file_name)\n",
        "\n",
        "  except Exception as e:\n",
        "    raise e\n",
        "\n",
        "  #######################################################################\n",
        "  #######################################################################\n",
        "hamilton=True\n",
        "\n",
        "if use_CLIPIT==False and use_zoom==False:\n",
        "  mse_decay = 0\n",
        "\n",
        "  if use_mse == False:\n",
        "      mse_init_weight = 0.\n",
        "  else:\n",
        "      mse_decay = mse_init_weight / mse_epoches\n",
        "    \n",
        "  if os.path.isdir('/content/drive') == False:\n",
        "      if save_to_drive == True or init_image_in_drive == True:\n",
        "          drive.mount('/content/drive')\n",
        "\n",
        "  if seed == -1:\n",
        "      seed = None\n",
        "  if init_image == \"None\":\n",
        "      init_image = None\n",
        "  if target_images == \"None\" or not target_images:\n",
        "      target_images = []\n",
        "  else:\n",
        "      target_images = target_images.split(\"|\")\n",
        "      target_images = [image.strip() for image in target_images]\n",
        "\n",
        "  prompts = [phrase.strip() for phrase in prompts.split(\"|\")]\n",
        "  if prompts == ['']:\n",
        "      prompts = []\n",
        "\n",
        "  altprompts = [phrase.strip() for phrase in altprompts.split(\"|\")]\n",
        "  if altprompts == ['']:\n",
        "      altprompts = []\n",
        "\n",
        "  if mse_images_interval == 0: mse_images_interval = images_interval\n",
        "  if mse_step_size == 0: mse_step_size = step_size\n",
        "  if mse_cutn == 0: mse_cutn = cutn\n",
        "  if mse_cut_pow == 0: mse_cut_pow = cut_pow\n",
        "  if alt_cut_pow == 0: alt_cut_pow = cut_pow\n",
        "  if alt_mse_cut_pow == 0: alt_mse_cut_pow = mse_cut_pow\n",
        "\n",
        "  augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomGaussianBlur((3,3),(4.5,4.5),p=0.3),\n",
        "            #K.RandomGaussianNoise(p=0.5),\n",
        "            #K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "            K.RandomPerspective(0.2,p=0.4, ),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),)\n",
        "\n",
        "  altaugs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            #K.RandomRotation(degrees=90.0),\n",
        "            K.RandomVerticalFlip(p=1),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomGaussianBlur((3,3),(4.5,4.5),p=0.3),\n",
        "            #K.RandomGaussianNoise(p=0.5),\n",
        "            #K.RandomElasticTransform(kernel_size=(33, 33), sigma=(7,7), p=0.2),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "            K.RandomPerspective(0.2,p=0.4, ),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),)\n",
        "\n",
        "  args = argparse.Namespace(\n",
        "      prompts=prompts,\n",
        "      altprompts=altprompts,\n",
        "      image_prompts=target_images,\n",
        "      noise_prompt_seeds=[],\n",
        "      noise_prompt_weights=[],\n",
        "      size=[width, height],\n",
        "      init_image=init_image,\n",
        "      png=transparent_png,\n",
        "      init_weight= mse_init_weight,\n",
        "      clip_model=clip_model,\n",
        "      vqgan_model=model_names[model],\n",
        "      step_size=step_size,\n",
        "      final_step_size = final_step_size,\n",
        "      cutn=cutn,\n",
        "      cut_pow=cut_pow,\n",
        "      mse_cutn = mse_cutn,\n",
        "      mse_cut_pow = mse_cut_pow,\n",
        "      mse_step_size = mse_step_size,\n",
        "      display_freq=images_interval,\n",
        "      mse_display_freq = mse_images_interval,\n",
        "      max_iterations=iterations,\n",
        "      mse_end = mse_decay_rate * mse_epoches,\n",
        "      seed=seed,\n",
        "      folder_name=folder_name,\n",
        "      save_to_drive=save_to_drive,\n",
        "      mse_decay_rate = mse_decay_rate,\n",
        "      mse_decay = mse_decay,\n",
        "      mse_with_zeros = mse_with_zeros,\n",
        "      normal_flip_optim = normal_flip_optim,\n",
        "      ema_val = ema_val,\n",
        "      augs = augs,\n",
        "      altaugs = altaugs,\n",
        "      alt_cut_pow = alt_cut_pow,\n",
        "      alt_mse_cut_pow = alt_mse_cut_pow,\n",
        "      is_gumbel = is_gumbel,\n",
        "  )\n",
        "\n",
        "  if save_to_drive == True:\n",
        "      if os.path.isdir('/content/drive/MyDrive/VQGAN_Output/'+folder_name) == False:\n",
        "          os.makedirs('/content/drive/MyDrive/VQGAN_Output/'+folder_name)\n",
        "  elif os.path.isdir('/content/'+folder_name) == False:\n",
        "      os.makedirs('/content/'+folder_name)\n",
        "\n",
        "\n",
        "  mh = ModelHost(args)\n",
        "  x= 0\n",
        "\n",
        "  for x in range(batch_size):\n",
        "      mh.setup_model(x)\n",
        "      last_iter = mh.run(x)\n",
        "      x=x+1\n",
        "\n",
        "  if batch_size != 1:\n",
        "    #clear_output()\n",
        "    print(\"===============================================================================\")\n",
        "    q = 0\n",
        "    while q < batch_size:\n",
        "      display(Image('/content/' + folder_name + \"/\" + str(q) + '.png'))\n",
        "      print(\"Image\" + str(q) + '.png')\n",
        "      q += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz9JTn67Bu2Y"
      },
      "source": [
        "### Saving Utilities <font size=\"+2\">🔱</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmK0k5zQeT5u",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🎥</font> Generate a video with the result <font size=\"+2\">🎥</font>\n",
        "#@markdown If you want to generate a video with the frames, just click on below. You can modify the min/max FPS, target length, the first frame or the last frame (-1 for end) below.\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "first_frame = 0 #@param {type:\"number\"}\n",
        "last_frame = -1 #@param {type:\"number\"}\n",
        "\n",
        "min_fps = 10 #@param {type:\"number\"}\n",
        "max_fps = 24 #@param {type:\"number\"}\n",
        "length = 15 #@param {type:\"number\"}\n",
        "\n",
        "end_frame = last_frame if last_frame != -1 else last_iter\n",
        "total_frames = end_frame-first_frame\n",
        "\n",
        "\n",
        "frames = []\n",
        "tqdm.write('Generating video...')\n",
        "for i in range(first_frame,end_frame): #\n",
        "    frames.append(Image.open(\"./steps/\"+ str(i) +'.png'))\n",
        "\n",
        "#fps = last_frame/10\n",
        "fps = np.clip(total_frames/length,min_fps,max_fps)\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "p = Popen(['ffmpeg', '-y', '-f', 'image2pipe', '-vcodec', 'png', '-r', str(fps), '-i', '-', '-vcodec', 'libx264', '-r', str(fps), '-pix_fmt', 'yuv420p', '-crf', '17', '-preset', 'veryslow', 'video.mp4'], stdin=PIPE)\n",
        "for im in tqdm(frames):\n",
        "    im.save(p.stdin, 'PNG')\n",
        "p.stdin.close()\n",
        "p.wait()\n",
        "\n",
        "#Crashes Colab!\n",
        "#from IPython.core.display import display, HTML\n",
        "#\n",
        "#mp4 = open('video.mp4','rb').read()\n",
        "#data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "#display(HTML(\"\"\"\n",
        "#<video width=400 controls>\n",
        "#      <source src=\"%s\" type=\"video/mp4\">\n",
        "#</video>\n",
        "#\"\"\" % data_url))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvWCfmjQKu66",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🧊</font> Super Slomo <font size=\"+2\">🧊</font>\n",
        "\n",
        "Slomo_Factor = 3 #@param {type:\"number\"}\n",
        "Target_FPS = 12 #@param {type:\"number\"}\n",
        "\n",
        "cmd1 = [\n",
        "    'python',\n",
        "    'Super-SloMo/video_to_slomo.py',\n",
        "    '--checkpoint',\n",
        "    pretrained_model,\n",
        "    '--video',\n",
        "    filepath,\n",
        "    '--sf',\n",
        "    str(Slomo_Factor),\n",
        "    '--fps',\n",
        "    str(Target_FPS),\n",
        "    '--output',\n",
        "    f'{filepath}-slomo.mkv',\n",
        "]\n",
        "process = subprocess.Popen(cmd1, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    raise RuntimeError(stderr)\n",
        "\n",
        "#I imagine death so much it feels more like a memory.\n",
        "\n",
        "cmd2 = [\n",
        "    'ffmpeg',\n",
        "    '-i',\n",
        "    f'{filepath}-slomo.mkv',\n",
        "    f'{filepath}-slomo.mp4',\n",
        "]\n",
        "\n",
        "process = subprocess.Popen(cmd2, cwd=f'/content', stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "stdout, stderr = process.communicate()\n",
        "if process.returncode != 0:\n",
        "    raise RuntimeError(stderr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT6Qc4xySMOq",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🎇</font> Increase Resolution <font size=\"+2\">🎇</font>\n",
        "#@markdown Uses SRCNN to significantly increase the resolution.\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "upload_folder = 'steps' #@param {type:\"string\"}\n",
        "result_folder = 'results' #@param {type:\"string\"}\n",
        "\n",
        "if os.path.isdir(upload_folder):\n",
        "    shutil.rmtree(upload_folder)\n",
        "if os.path.isdir(result_folder):\n",
        "    shutil.rmtree(result_folder)\n",
        "os.mkdir(upload_folder)\n",
        "os.mkdir(result_folder)\n",
        "\n",
        "# upload images\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "  dst_path = os.path.join(upload_folder, filename)\n",
        "  print(f'move {filename} to {dst_path}')\n",
        "  shutil.move(filename, dst_path)\n",
        "\n",
        "\n",
        "# if it is out of memory, try to use the `--tile` option\n",
        "# We upsample the image with the scale factor X3.5\n",
        "!python inference_realesrgan.py --model_path experiments/pretrained_models/RealESRGAN_x4plus.pth --input upload --netscale 4 --outscale 3.5 --half --face_enhance\n",
        "# Arguments\n",
        "# --model_path: the path to the pretrained model\n",
        "# --input: input folder or image\n",
        "# --netscale: The scale factor in the network architecture. It should be consistent with the model\n",
        "# --outscale: Output scale, can be arbitrary scale factore. \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpobRa2dB7Cn",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🔗</font> Mount Google Drive <font size=\"+2\">🔗</font>\n",
        "#@markdown Required to transfer steps to drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ESgXEymB-Jp",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">📁</font> Zip/Transfer Steps to Drive <font size=\"+2\">📁</font>\n",
        "#@markdown This will zip up all steps images and the seed/rules used and transfer it to your drive (make sure you mount it above first)\n",
        "\n",
        "#@markdown Note that the default windows zip utility seems to have trouble with these archives and I have no idea why--I'd recommend using 7zip instead\n",
        "\n",
        "#@markdown (I didn't edit this code, it's just here for posteriority's sake)\n",
        "\n",
        "import datetime, zipfile, shutil, json\n",
        "\n",
        "file_safe_characters = set('_-')\n",
        "file_repl_rules = {' ':'-'}\n",
        "\n",
        "def timestamp():\n",
        "  time = datetime.datetime.now()\n",
        "  return time.strftime('%Y%m%d%H%M%S')\n",
        "def safe_filename(name):\n",
        "  for find,repl in file_repl_rules.items():\n",
        "    name = name.replace(find, repl)\n",
        "  return \"\".join(c for c in name if c.isalnum() or c in file_safe_characters).rstrip()\n",
        "\n",
        "base_name = '{}-{}'.format(timestamp(),safe_filename('|'.join(args.prompts))[:30])\n",
        "zf = zipfile.ZipFile(base_name+'.zip', 'w', compression=zipfile.ZIP_DEFLATED, compresslevel=9)\n",
        "\n",
        "arg_dict = {'iterations':last_iter}\n",
        "arg_dict.update(mh.metadata)\n",
        "zf.writestr('rules.json',json.dumps(arg_dict,indent=2))\n",
        "\n",
        "for i in range(0,last_iter):\n",
        "  zf.write(f'steps/{i}.png')\n",
        "\n",
        "shutil.copy(base_name+'.zip', 'drive/MyDrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9VpgIGi_utC"
      },
      "source": [
        "### Other Utilities <font size=\"+2\">🌟</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRDcexDhMOJN",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🚮</font> Clear *Steps* folder <font size=\"+2\">🚮</font>\n",
        "path = f'/content/steps'\n",
        "!rm -r {path}\n",
        "!mkdir --parents {path}\n",
        "\n",
        "#Congress, I beg of you, justify your existence. pwease"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9PRm0PyIvsC",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🪐</font> Prompt Builders <font size=\"+2\">🪐</font>\n",
        "#@markdown A builder to enhance your prompt. Customize your prompt and run this cell. You can select multiple items\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "Your_Prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown\n",
        "Generally_better = True #@param {type:\"boolean\"}\n",
        "Realistic = False #@param {type:\"boolean\"}\n",
        "HD_3D = False #@param {type:\"boolean\"}\n",
        "Painting = False #@param {type:\"boolean\"}\n",
        "Crazy = False #@param {type:\"boolean\"}\n",
        "Retro = False #@param {type:\"boolean\"}\n",
        "Sketch = False #@param {type:\"boolean\"}\n",
        "Comics = False #@param {type:\"boolean\"}\n",
        "Aesthetic = False #@param {type:\"boolean\"}\n",
        "Experimental = False #@param {type:\"boolean\"}\n",
        "\n",
        "generally_better_preset = \"\"\n",
        "realistic_preset = \"\"\n",
        "HD_3D_preset = \"\"\n",
        "painting_preset = \"\"\n",
        "crazy_preset = \"\"\n",
        "retro_preset = \"\"\n",
        "sketch_preset = \"\"\n",
        "comics_preset = \"\"\n",
        "aesthetic_preset = \"\"\n",
        "experimental_preset = \"\"\n",
        "\n",
        "#Hey you, yes you. If you are wondering why there are Hamilton references all over you place, well--\n",
        "\n",
        "if Generally_better:\n",
        "    generally_better_preset = \" | 8K | trending on artstation\"\n",
        "if Realistic:\n",
        "    realistic_preset = \" | Flickr | filmic | CryEngine\"\n",
        "if HD_3D:\n",
        "    HD_3D_preset = \" | Rendered in Cinema4D | 8K 3D | CGSociety | ZBrush\"\n",
        "if Painting:\n",
        "    painting_preset = \" | oil on canvas | Impressionism | by James Gurney\"\n",
        "if Crazy:\n",
        "    crazy_preset = \" | Fauvism | by Thomas Kinkade | psychedelic\"\n",
        "if Retro:\n",
        "    retro_preset = \" | Tri-X 400 TX | (1962) directed by cinematography by | #film\"\n",
        "if Sketch:\n",
        "    sketch_preset = \" | stipple | charcoal drawing | ink drawing\"\n",
        "if Comics:\n",
        "    comics_preset = \" | Marvel Comics | Booru | flat shading\"\n",
        "if Aesthetic:\n",
        "    aesthetic_preset = \" | Art on Instagram | matte background | digital illustration\"\n",
        "if Experimental:\n",
        "    experimental_preset = \" | groovy | pop art | Lowbrow | rough\"\n",
        "\n",
        "New_Prompt = Your_Prompt + generally_better_preset + realistic_preset + HD_3D_preset + painting_preset + crazy_preset + retro_preset + sketch_preset + comics_preset + aesthetic_preset + experimental_preset\n",
        "\n",
        "printmd(\"<b>New prompt: </b>\" + New_Prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDHgtuxWI8-w",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🚀</font> Prompt Speed Estimator <font size=\"+2\">🚀</font>\n",
        "#@markdown This is very rough and theoretical estimating.\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "def get_gpu():\n",
        "  GPU = !nvidia-smi -L\n",
        "  GPU = str(GPU).replace(\"GPU 0:\", \"\")\n",
        "  GPU = str(GPU).replace(\"[' Tesla \", \"\")\n",
        "  split_GPU = str(GPU).split(\" (UUID\", 1)\n",
        "  GPU = split_GPU[0]\n",
        "  return GPU\n",
        "\n",
        "Your_Prompt = \"\" #@param {type:\"string\"}\n",
        "\n",
        "speed = len(Your_Prompt)\n",
        "\n",
        "if get_gpu() == \"V100\":\n",
        "  speed += 1\n",
        "elif get_gpu() == \"P100\":\n",
        "  speed += 4\n",
        "elif get_gpu() == \"T4\":\n",
        "  speed += 7\n",
        "elif get_gpu() == \"K80\":\n",
        "  speed += 13\n",
        "elif get_gpu() == \"P4\":\n",
        "  speed += 20\n",
        "\n",
        "#Scratch that this is not a moment it's the movement!\n",
        "\n",
        "q = 0\n",
        "for i in Your_Prompt:\n",
        "    if i == '|':\n",
        "        q += 1\n",
        "speed += q*6\n",
        "\n",
        "if speed < 6:\n",
        "  printmb(\"Speed: <b>Mega Fast!</b>\")\n",
        "elif len(Your_Prompt) < 6:\n",
        "  print(\"Speed: Mega Fast!\")\n",
        "elif speed < 60:\n",
        "  printmd(\"Speed: <b>Fast</b>\")\n",
        "elif speed < 85:\n",
        "  printmd(\"Speed: <b>Medium</b>\")\n",
        "elif speed < 110:\n",
        "  printmd(\"Speed: <b>Slow</b>\")\n",
        "elif speed < 125:\n",
        "  printmd(\"Speed: <b>Very Slow!</b>\")\n",
        "elif speed < 200:\n",
        "  printmd(\"Speed: <b>Mega Slow!</b>\")\n",
        "\n",
        "#print(speed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsC9iZXCJBKd",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🌆</font> Random init image <font size=\"+2\">🌆</font>\n",
        "#@markdown Configure and run this cell. If you don't change the name of the file, every new image will replace the previous one.\n",
        "#from IPython.display import Image, clear_output\n",
        "#import urllib.request\n",
        "#import random\n",
        "\n",
        "%mkdir Init_Img\n",
        "\n",
        "clear_output()\n",
        "\n",
        "#An open letter to the fat, arrogant, anti-charismatic, national embarrassment known was President John Adams.\n",
        "\n",
        "Width = 480 #@param {type:\"number\"}\n",
        "Height = 336 #@param {type:\"number\"}\n",
        "\n",
        "url = \"https://picsum.photos/\" + str(Width) + \"/\" + str(Height) + \"?blur=\" + str(random.randrange(5, 10))\n",
        "\n",
        "Name = \"Image.png\" #@param {type:\"string\"}\n",
        "Folder = \"Init_Img\" #@param {type:\"string\"}\n",
        "\n",
        "urllib.request.urlretrieve(url, Folder + \"/\" + Name)\n",
        "Image(Folder + \"/\" + Name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5s7fjBkyMzM",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">👓</font> Metadata Reader <font size=\"+2\">👓</font>\n",
        "#@markdown upload an image and put its name below or put a url to an image to read its metadata\n",
        "from PIL.PngImagePlugin import PngImageFile, PngInfo\n",
        "import json, io, requests, re\n",
        "def get_local(path):\n",
        "  return PngImageFile(path)\n",
        "def get_remote(url):\n",
        "  with requests.get(url, stream=True).raw as stream:\n",
        "    imdat = io.BytesIO(stream.read())\n",
        "    return PngImageFile(imdat)\n",
        "image = \"/content/\" #@param {type:\"string\"}\n",
        "if (re.match(r'^https?://', image)):\n",
        "  targetImage = get_remote(image)\n",
        "else:\n",
        "  targetImage = get_local(image)\n",
        "dat = json.loads(targetImage.text['vqgan-params'])\n",
        "print(json.dumps(dat,indent=2))\n",
        "#@markdown note that images must be _downloaded_ from the notebook, not copy-pasted (this strips the metadata) to be readable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptdikgiH_wqw",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🧮</font> Resolution Calculator <font size=\"+2\">🧮</font>\n",
        "\n",
        "from fractions import Fraction as frac\n",
        "import tabulate\n",
        "\n",
        "def find_ratios(x, y, threshold=0.01):\n",
        "  r = float(frac(x)/frac(y))\n",
        "  options = []\n",
        "  for w in range(1,101):\n",
        "    h = round(w/r)\n",
        "    if (h == 0): continue\n",
        "    if abs((r-w/h)/r) < threshold:\n",
        "      options.append((w*16, h*16, (r-w/h)/r)) \n",
        "  return options\n",
        "\n",
        "px_units = [('MP',1e6),('kP',1e3)]\n",
        "def render_pixel(px_ct, precision=1):\n",
        "  for unit,size in px_units:\n",
        "    if px_ct > size:\n",
        "      return ('{:.'+str(precision)+'f} {}').format(px_ct/size, unit)\n",
        "\n",
        "def format_options(options):\n",
        "  return [(\n",
        "    '{} x {}'.format(x[0],x[1]),\n",
        "    abs(x[2]),\n",
        "    render_pixel(x[0]*x[1])) for x in options]\n",
        "  return '{} P'.format(px_ct)\n",
        "\n",
        "def present_ratios(options, great=0.005, max_size=500000):\n",
        "  perfect = []\n",
        "  options = [x for x in options if x[0]*x[1] <= max_size]\n",
        "  for option in options:\n",
        "    if (option[2] == 0):\n",
        "      res = '{} x {}'.format(option[0],option[1])\n",
        "      px_ct = render_pixel(option[0]*option[1])\n",
        "      perfect.append((res, px_ct))\n",
        "  options.sort(key = lambda x:abs(x[2]))\n",
        "  great_fits = [x for x in options if x[2] <= great and x[2] != 0]\n",
        "  best_great_fits = []\n",
        "  n = 0\n",
        "  for i in great_fits:\n",
        "    best_great_fits.append(i)\n",
        "    n += 1\n",
        "    if (n == 10): break\n",
        "    if (i[0] * i[1] > 1e6): break\n",
        "  great_fits = best_great_fits\n",
        "  great_fits.sort(key = lambda x : x[0]*x[1])\n",
        "  great_fits = format_options(great_fits)\n",
        "  options.sort(key = lambda x : x[0]*x[1])\n",
        "  small_fits = format_options([x for x in options if x[0]*x[1] < 200000 and x[2] != 0])\n",
        "  med_fits = format_options([x for x in options if 200000 <= x[0]*x[1] < 500000 and x[2] != 0])\n",
        "  large_fits = format_options([x for x in options if 500000 <= x[0]*x[1] and x[2] > great])\n",
        "  print(tabulate.tabulate(perfect,headers=('Perfect Fits', 'Pixels')))\n",
        "  print()\n",
        "  print(tabulate.tabulate(great_fits,headers=('Great Fits', 'Error', 'Pixels')))\n",
        "  print()\n",
        "  print(tabulate.tabulate(small_fits, headers=('Small Sizes', 'Error', 'Pixels')))\n",
        "  print()\n",
        "  print(tabulate.tabulate(med_fits, headers=('Med Sizes', 'Error', 'Pixels')))\n",
        "  print()\n",
        "  print(tabulate.tabulate(large_fits, headers=('Large Sizes', 'Error', 'Pixels')))\n",
        "\n",
        "#@markdown This will calculate good resolutions for a given aspect ratio. You can give any common numeric form like fractions, decimals, or integers for each part of the ratio.\n",
        "width = \"4050\" #@param {type:\"string\"}\n",
        "height = \"6000\" #@param {type:\"string\"}\n",
        "max_size = 600000 #@param {type:\"number\"}\n",
        "\n",
        "present_ratios(find_ratios(width,height), max_size=max_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLG0B-YvkIbm",
        "cellView": "form"
      },
      "source": [
        "#@title <font size=\"+2\">🥥</font> Import Coco from Google Drive <font size=\"+2\">🥥</font>\n",
        "#@markdown If you have coco.ckpt saved to the root of your google drive, this will copy it over.\n",
        "#@markdown Run this before installing the models to save time.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/coco.ckpt /content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "UwU"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}